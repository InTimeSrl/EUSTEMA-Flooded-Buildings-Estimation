{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0cefa9",
   "metadata": {},
   "source": [
    "# Integrazione wd_estimation.py con Dataiku DSS\n",
    "\n",
    "Implementazione dello script di analisi della sommersione degli edifici (`wd_estimation.py`) in un recipe Python di Dataiku DSS.\n",
    "\n",
    "## Obiettivo\n",
    "Calcolare la percentuale di sommersione degli edifici durante eventi alluvionali analizzando la profondit√† dell'acqua nei pixel esterni al perimetro di ciascun edificio.\n",
    "\n",
    "## üé≠ **Modalit√† di Esecuzione**\n",
    "\n",
    "Il sistema supporta due modalit√† operative con rilevamento automatico:\n",
    "\n",
    "### **üë§ ESECUZIONE MANUALE**\n",
    "- Operatore modifica dataset `configurazione_parametri`\n",
    "- Esecuzione manuale del recipe\n",
    "- Sistema legge parametri dai dataset Dataiku\n",
    "\n",
    "### **ü§ñ ESECUZIONE AUTOMATICA (Scenario Avanzato)**\n",
    "- Sistema attivato automaticamente (timer, trigger, API)\n",
    "- Parametri passati tramite **JSON Scenario Variables**\n",
    "- **Integrazione automatica** con dataset Dataiku per parametri mancanti\n",
    "\n",
    "## ‚öñÔ∏è **Sistema di Priorit√† dei Parametri**\n",
    "\n",
    "Il sistema implementa una gerarchia di priorit√† dei parametri:\n",
    "\n",
    "1. **ü•á Scenario JSON** (Priorit√† MASSIMA)\n",
    "   - Parametri espliciti nel JSON scenario sovrascrivono tutto\n",
    "   \n",
    "2. **ü•à Dataset Dataiku** (Priorit√† MEDIA) \n",
    "   - Integrazione automatica per parametri mancanti nel JSON\n",
    "   - `configurazione_parametri` e `configurazione_dati`\n",
    "   \n",
    "3. **ü•â Valori Default** (Priorit√† MINIMA)\n",
    "   - Fallback finale per parametri non specificati\n",
    "\n",
    "**üí° ESEMPIO PRATICO**: \n",
    "- JSON fornisce solo `files` ‚Üí Sistema integra automaticamente `HEIGHT_FIELD`, `EPSG`, etc. dai dataset Dataiku\n",
    "- JSON fornisce parametri completi ‚Üí Usa solo quelli specificati, ignora dataset\n",
    "\n",
    "### **üìã Esempi JSON Scenario Variables**\n",
    "\n",
    "#### **üü¢ Scenario MINIMO (Auto-integrazione):**\n",
    "```json\n",
    "{\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici.shp\", \n",
    "    \"raster\": \"flood_depth.tif\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "*‚Üí Sistema integra automaticamente tutti i parametri mancanti dai dataset Dataiku*\n",
    "\n",
    "#### **üîß Scenario COMPLETO (Override totale):**\n",
    "```json\n",
    "{\n",
    "  \"elab_id\": \"flood_analysis_complete_001\",\n",
    "  \"event_name\": \"Alluvione Tevere Ottobre 2024\",\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici_roma.shp\",\n",
    "    \"raster\": \"flood_depth_tevere.tif\"\n",
    "  },\n",
    "  \"HEIGHT_FIELD\": \"H_UVL\",\n",
    "  \"TARGET_EPSG\": \"32632\",\n",
    "  \"REPROJECTION_OPTION\": 1,\n",
    "  \"BUFFER_DISTANCE\": \"auto\",\n",
    "  \"min_valid_height\": 0.5,\n",
    "  \"enable_logging\": true,\n",
    "  \"create_report\": false,\n",
    "  \"create_shapefile\": true,\n",
    "  \"output_naming\": {\n",
    "    \"dataset_name\": \"risultati_alluvione_tevere\",\n",
    "    \"folder_name\": \"output_tevere_analisi\",\n",
    "    \"file_prefix\": \"tevere_\",\n",
    "    \"file_suffix\": \"final\",\n",
    "    \"include_timestamp\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "*‚Üí Usa solo parametri JSON, ignora dataset Dataiku*\n",
    "\n",
    "#### **üé® Scenario NAMING PERSONALIZZATO (Controllo completo output):**\n",
    "```json\n",
    "{\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici_tevere.shp\",\n",
    "    \"raster\": \"flood_depth_tevere.tif\"\n",
    "  },\n",
    "  \"event_name\": \"Alluvione_Tevere_2024\",\n",
    "  \"output_naming\": {\n",
    "    \"dataset_name\": \"risultati_alluvione_tevere\",\n",
    "    \"folder_name\": \"output_tevere_analisi\",\n",
    "    \"file_prefix\": \"tevere_flood_\",\n",
    "    \"file_suffix\": \"final\",\n",
    "    \"include_timestamp\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "*‚Üí Personalizza completamente nomi di dataset, folder e file output*\n",
    "\n",
    "**üöÄ Il sistema si adatta automaticamente alla modalit√† di esecuzione**\n",
    "\n",
    "## Input Dataiku\n",
    "- **Dataset**: `configurazione_parametri` - Contiene i parametri di configurazione (HEIGHT_FIELD, REPROJECTION_OPTION, TARGET_EPSG, BUFFER_DISTANCE)\n",
    "- **Dataset**: `configurazione_dati` - Contiene la selezione dei file di input (tipo_file, nome_file) per file vettoriali e raster specifici\n",
    "- **Folder**: `minio_input` - Folder Minio con file geospaziali organizzati in sottocartelle\n",
    "\n",
    "## Formati Supportati\n",
    "### üìÇ **Formati Vettoriali**:\n",
    "- **Shapefile** (`.shp`) - formato standard ESRI con file accessori\n",
    "- **GeoJSON** (`.geojson`, `.json`) - formato JSON geografico\n",
    "- **GeoPackage** (`.gpkg`) - formato moderno OGC \n",
    "- **GeoParquet** (`.parquet`, `.geoparquet`) - formato colonnare ottimizzato per performance\n",
    "- **KML** (`.kml`) - formato Google Earth\n",
    "- **GML** (`.gml`) - Geography Markup Language\n",
    "\n",
    "### üó∫Ô∏è **Formati Raster**:\n",
    "- **GeoTIFF** (`.tif`, `.tiff`) - formato standard georeferenziato\n",
    "- **ERDAS Imagine** (`.img`) - formato imaging professionale\n",
    "- **JPEG2000** (`.jp2`) - compressione avanzata con georiferimento\n",
    "- **Immagini standard** (`.png`, `.jpg`, `.jpeg`, `.bmp`, `.gif`) - con world file\n",
    "\n",
    "## üéõÔ∏è **Controlli di Output**\n",
    "\n",
    "Il sistema supporta controlli granulari per personalizzare l'output:\n",
    "\n",
    "- **`enable_logging`**: Attiva/disattiva logging dettagliato (default: `true`)\n",
    "- **`create_report`**: Genera report statistico HTML (default: `true`) \n",
    "- **`create_shapefile`**: Salva risultati come shapefile (default: `true`)\n",
    "\n",
    "**Esempio controllo output**:\n",
    "```json\n",
    "{\n",
    "  \"files\": { \"vettoriale\": \"edifici.shp\", \"raster\": \"flood.tif\" },\n",
    "  \"enable_logging\": false,\n",
    "  \"create_report\": true,  \n",
    "  \"create_shapefile\": false\n",
    "}\n",
    "```\n",
    "\n",
    "## Output Dataiku  \n",
    "- **Dataset**: `output_inondazioni` - DataFrame con risultati dell'analisi di sommersione\n",
    "- **Folder**: `output_inondazioni` - Folder con shapefile risultanti, report statistici e file di log (condizionali)\n",
    "\n",
    "## üß™ **Framework di Testing**\n",
    "\n",
    "Il notebook include un **sistema di test isolato** (celle finali) per validare scenari JSON senza interferire con il workflow principale:\n",
    "\n",
    "- **Test BASE**: JSON minimale con integrazione Dataiku\n",
    "- **Test COMPLETO**: JSON con tutti i parametri\n",
    "- **Test ERROR**: Validazione gestione errori\n",
    "- **Test NAMING**: Naming output personalizzabile\n",
    "- **Test CASE-INSENSITIVE**: Scenario con parametri insensibili a maiuscole/minuscole\n",
    "\n",
    "## Metodologia\n",
    "L'analisi utilizza la tecnica del **campionamento esterno dei pixel** per determinare la profondit√† dell'acqua attorno agli edifici, calcolando statistiche di sommersione basate sul rapporto tra profondit√† media dell'acqua e altezza dell'edificio.\n",
    "\n",
    "### ‚ö†Ô∏è **Nota Tecnica - Buffer Distance**\n",
    "Il parametro `BUFFER_DISTANCE` definisce la distanza (in metri) del buffer attorno agli edifici per il campionamento dei pixel:\n",
    "- **Automatico** (`auto`): usa la risoluzione spaziale del raster (consigliato)\n",
    "- **Manuale**: valore in metri - **IMPORTANTE**: deve essere ‚â• 50% della risoluzione pixel per risultati affidabili\n",
    "- **Esempio**: con risoluzione 1m, buffer < 0.5m pu√≤ produrre campioni insufficienti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afb239",
   "metadata": {},
   "source": [
    "## 1. Setup e Configurazione\n",
    "\n",
    "Import delle librerie necessarie e configurazione dei parametri dal dataset Dataiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import librerie di base\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "\n",
    "# Import librerie geospaziali\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from shapely.geometry import mapping\n",
    "from shapely.ops import unary_union\n",
    "import fiona\n",
    "\n",
    "# Import librerie di utilit√†\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import shutil\n",
    "import warnings\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "# Configurazione per sopprimere warning di librerie geospaziali\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Input shapes do not overlap raster.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*invalid value encountered.*\")\n",
    "\n",
    "# Classe per catturare le stampe SOLO nel log (senza output a schermo)\n",
    "class LogCapture:\n",
    "    def __init__(self):\n",
    "        self.log_buffer = StringIO()\n",
    "        self.original_stdout = sys.stdout\n",
    "        \n",
    "    def write(self, text):\n",
    "        # Scrivi SOLO nel buffer di log - niente a schermo\n",
    "        self.log_buffer.write(text)\n",
    "        \n",
    "    def flush(self):\n",
    "        self.log_buffer.flush()\n",
    "        \n",
    "    def get_log_content(self):\n",
    "        return self.log_buffer.getvalue()\n",
    "        \n",
    "    def clear_log(self):\n",
    "        self.log_buffer.close()\n",
    "        self.log_buffer = StringIO()\n",
    "        \n",
    "    def close(self):\n",
    "        self.log_buffer.close()\n",
    "\n",
    "# SISTEMA ERROR HANDLING ROBUSTO per FLOOD ANALYSIS\n",
    "class FloodAnalysisError(Exception):\n",
    "    \"\"\"Eccezione personalizzata per errori di flood analysis\"\"\"\n",
    "    pass\n",
    "\n",
    "class ErrorHandler:\n",
    "    \"\"\"\n",
    "    Gestore errori centralizzato per analisi sommersione\n",
    "    Traccia, categorizza e gestisce tutti gli errori del workflow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.errors = []\n",
    "        self.warnings = []\n",
    "        self.stats = {\n",
    "            'file_errors': 0,\n",
    "            'processing_errors': 0, \n",
    "            'validation_errors': 0,\n",
    "            'geometry_errors': 0,\n",
    "            'data_errors': 0\n",
    "        }\n",
    "    \n",
    "    def handle_file_error(self, operation: str, filename: str, error: Exception):\n",
    "        \"\"\"Gestisce errori di file I/O\"\"\"\n",
    "        msg = f\"File {operation} failed for {filename}: {str(error)}\"\n",
    "        self.errors.append(('FILE_ERROR', msg, error))\n",
    "        self.stats['file_errors'] += 1\n",
    "        print(f\"‚ùå FILE ERROR: {msg}\")\n",
    "        return False\n",
    "    \n",
    "    def handle_processing_error(self, building_id: str, operation: str, error: Exception):\n",
    "        \"\"\"Gestisce errori di processing degli edifici\"\"\"\n",
    "        msg = f\"Processing {operation} failed for building {building_id}: {str(error)}\"\n",
    "        self.errors.append(('PROCESSING_ERROR', msg, error))\n",
    "        self.stats['processing_errors'] += 1\n",
    "        # Non stampare - troppo verboso per migliaia di edifici\n",
    "        return False\n",
    "    \n",
    "    def handle_validation_error(self, validation_type: str, details: str):\n",
    "        \"\"\"Gestisce errori di validazione\"\"\"\n",
    "        msg = f\"Validation failed: {validation_type} - {details}\"\n",
    "        self.errors.append(('VALIDATION_ERROR', msg, None))\n",
    "        self.stats['validation_errors'] += 1\n",
    "        print(f\"‚ö†Ô∏è VALIDATION ERROR: {msg}\")\n",
    "        return False\n",
    "    \n",
    "    def handle_geometry_error(self, building_id: str, operation: str, error: Exception):\n",
    "        \"\"\"Gestisce errori geometrici\"\"\"\n",
    "        msg = f\"Geometry {operation} failed for building {building_id}: {str(error)}\"\n",
    "        self.errors.append(('GEOMETRY_ERROR', msg, error))\n",
    "        self.stats['geometry_errors'] += 1\n",
    "        return False\n",
    "    \n",
    "    def add_warning(self, warning_type: str, message: str):\n",
    "        \"\"\"Aggiunge warning non bloccante\"\"\"\n",
    "        self.warnings.append((warning_type, message))\n",
    "        print(f\"‚ö†Ô∏è WARNING: {message}\")\n",
    "    \n",
    "    def get_error_summary(self):\n",
    "        \"\"\"Ritorna summary degli errori per report\"\"\"\n",
    "        total_errors = len(self.errors)\n",
    "        summary = {\n",
    "            'total_errors': total_errors,\n",
    "            'total_warnings': len(self.warnings),\n",
    "            'stats': self.stats.copy(),\n",
    "            'has_critical_errors': any(error[0] in ['FILE_ERROR', 'VALIDATION_ERROR'] for error in self.errors)\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def should_continue_processing(self):\n",
    "        \"\"\"Determina se continuare processing nonostante errori\"\"\"\n",
    "        # Stop solo per errori critici (file, validazione)\n",
    "        critical_errors = [e for e in self.errors if e[0] in ['FILE_ERROR', 'VALIDATION_ERROR']]\n",
    "        return len(critical_errors) == 0\n",
    "    \n",
    "    def print_final_report(self):\n",
    "        \"\"\"Stampa report finale errori\"\"\"\n",
    "        print(f\"\\n=== ERROR HANDLING REPORT ===\")\n",
    "        print(f\"Total errors: {len(self.errors)}\")\n",
    "        print(f\"Total warnings: {len(self.warnings)}\")\n",
    "        for error_type, count in self.stats.items():\n",
    "            if count > 0:\n",
    "                print(f\"  {error_type}: {count}\")\n",
    "        \n",
    "        if self.warnings:\n",
    "            print(f\"\\nTop 5 warnings:\")\n",
    "            for i, (warn_type, msg) in enumerate(self.warnings[:5]):\n",
    "                print(f\"  {i+1}. [{warn_type}] {msg}\")\n",
    "\n",
    "# ATTIVA LOGGING GLOBALE PER CATTURARE TUTTE LE STAMPE\n",
    "# Ripristina stdout se gi√† attivo, poi ricrea il sistema di logging\n",
    "if 'log_capture' in globals():\n",
    "    sys.stdout = log_capture.original_stdout\n",
    "    log_capture.close()\n",
    "\n",
    "log_capture = LogCapture()\n",
    "sys.stdout = log_capture\n",
    "\n",
    "print(\"‚úÖ Tutte le librerie importate con successo\")\n",
    "print(\"üìù Sistema di logging attivato - OUTPUT NASCOSTO\")\n",
    "\n",
    "# Ripristina stdout per mostrare solo questo messaggio di conferma\n",
    "sys.stdout = log_capture.original_stdout\n",
    "print(\"üîá MODALIT√Ä SILENZIOSA ATTIVATA - Output nascosto (visibile solo nel log finale)\")\n",
    "sys.stdout = log_capture\n",
    "\n",
    "# Funzioni payload integrate per configurazione avanzata\n",
    "def _create_payload():\n",
    "    \"\"\"Crea payload unificato per analisi inondazioni\"\"\"\n",
    "    payload = {\n",
    "        \"elab_id\": f\"flood_{datetime.now(pytz.timezone('Europe/Rome')).strftime('%Y%m%d_%H%M%S')}\"\n",
    "    }\n",
    "    \n",
    "    # Leggi parametri da tabelle esistenti\n",
    "    try:\n",
    "        conf_params = dataiku.Dataset(\"configurazione_parametri\").get_dataframe()\n",
    "        for _, row in conf_params.iterrows():\n",
    "            payload[row['variabile']] = row['valore']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Leggi file da configurazione dati\n",
    "    try:\n",
    "        conf_data = dataiku.Dataset(\"configurazione_dati\").get_dataframe()\n",
    "        payload['files'] = {}\n",
    "        for _, row in conf_data.iterrows():\n",
    "            payload['files'][row['tipo_file']] = row['nome_file']\n",
    "    except:\n",
    "        payload['files'] = {}\n",
    "    \n",
    "    return payload\n",
    "\n",
    "def _download_remote_to_tmp(file_path: str, folder_obj, tmpdir: str):\n",
    "    \"\"\"\n",
    "    Scarica file da Dataiku Folder in directory temporanea\n",
    "    Versione adattata per flood analysis con file semplici\n",
    "    \n",
    "    file_path: percorso del file nel folder\n",
    "    folder_obj: oggetto dataiku.Folder\n",
    "    tmpdir: directory locale di destinazione\n",
    "    Restituisce percorso locale al file scaricato oppure None se non trovato.\n",
    "    \"\"\"\n",
    "    if not file_path:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Gestisci tmpdir come string o oggetto TemporaryDirectory\n",
    "        tmpdir_path = tmpdir.name if hasattr(tmpdir, \"name\") else str(tmpdir)\n",
    "        \n",
    "        # Ottieni lista file nel folder\n",
    "        try:\n",
    "            file_list = folder_obj.list_paths_in_partition()\n",
    "        except Exception:\n",
    "            file_list = []\n",
    "        \n",
    "        # 1) Match esatto per il percorso\n",
    "        if file_path in file_list:\n",
    "            local_path = os.path.join(tmpdir_path, os.path.basename(file_path))\n",
    "            with folder_obj.get_download_stream(file_path) as stream, open(local_path, 'wb') as out:\n",
    "                out.write(stream.read())\n",
    "            \n",
    "            # Se √® uno shapefile, scarica anche i file accessori\n",
    "            if file_path.lower().endswith('.shp'):\n",
    "                base_name = os.path.splitext(file_path)[0]\n",
    "                shapefile_extensions = ['.dbf', '.shx', '.prj', '.qix', '.xml', '.cpg', '.sbx', '.sbn']\n",
    "                \n",
    "                for ext in shapefile_extensions:\n",
    "                    aux_file = base_name + ext\n",
    "                    if aux_file in file_list:\n",
    "                        aux_local_path = os.path.join(tmpdir_path, os.path.basename(aux_file))\n",
    "                        try:\n",
    "                            with folder_obj.get_download_stream(aux_file) as stream, open(aux_local_path, 'wb') as out:\n",
    "                                out.write(stream.read())\n",
    "                        except Exception:\n",
    "                            continue\n",
    "            \n",
    "            return local_path\n",
    "        \n",
    "        # 2) Match per nome file (case-insensitive)\n",
    "        filename = os.path.basename(file_path)\n",
    "        for f in file_list:\n",
    "            if os.path.basename(f).lower() == filename.lower():\n",
    "                local_path = os.path.join(tmpdir_path, os.path.basename(f))\n",
    "                with folder_obj.get_download_stream(f) as stream, open(local_path, 'wb') as out:\n",
    "                    out.write(stream.read())\n",
    "                \n",
    "                # Se √® uno shapefile, scarica anche i file accessori\n",
    "                if f.lower().endswith('.shp'):\n",
    "                    base_name = os.path.splitext(f)[0]\n",
    "                    shapefile_extensions = ['.dbf', '.shx', '.prj', '.qix', '.xml', '.cpg', '.sbx', '.sbn']\n",
    "                    \n",
    "                    for ext in shapefile_extensions:\n",
    "                        aux_file = base_name + ext\n",
    "                        if aux_file in file_list or aux_file.lower() in [x.lower() for x in file_list]:\n",
    "                            # Trova il file con case corretto\n",
    "                            actual_aux_file = next((x for x in file_list if x.lower() == aux_file.lower()), None)\n",
    "                            if actual_aux_file:\n",
    "                                aux_local_path = os.path.join(tmpdir_path, os.path.basename(actual_aux_file))\n",
    "                                try:\n",
    "                                    with folder_obj.get_download_stream(actual_aux_file) as stream, open(aux_local_path, 'wb') as out:\n",
    "                                        out.write(stream.read())\n",
    "                                except Exception:\n",
    "                                    continue\n",
    "                \n",
    "                return local_path\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore download {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Funzioni payload integrate e Sistema Error Handling definito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e47406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSE CONFIGURAZIONE FLOOD ANALYSIS\n",
    "class FloodAnalysisConfig:\n",
    "    \"\"\"\n",
    "    Configurazione centralizzata per analisi sommersione edifici.\n",
    "    \n",
    "    Gestisce parametri operativi, validazione input e compatibilit√†\n",
    "    con sistema payload per automazione.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, payload: dict = None):\n",
    "        \"\"\"\n",
    "        Inizializza configurazione con priorit√†:\n",
    "        1. Scenario JSON (se presente) - PRIORIT√Ä MASSIMA\n",
    "        2. Dataset Dataiku 'configurazione_parametri' (primo fallback) - PRIORIT√Ä MEDIA\n",
    "        3. Defaults classe (solo se payload e dataset Dataiku mancanti) - PRIORIT√Ä MINIMA\n",
    "        \n",
    "        Nota: Lo scenario JSON sovrascrive COMPLETAMENTE i dataset Dataiku\n",
    "        \"\"\"\n",
    "        \n",
    "        # DEFAULTS SOLO COME FALLBACK (non sovrascrivono dataset Dataiku)\n",
    "        self.HEIGHT_FIELD = None       # Deve essere definito nel dataset!\n",
    "        self.REPROJECTION_OPTION = None  # Deve essere definito nel dataset!\n",
    "        self.TARGET_EPSG = None        # Deve essere definito nel dataset!\n",
    "        self.BUFFER_DISTANCE = None   # Deve essere definito nel dataset!\n",
    "        \n",
    "        # Formati supportati\n",
    "        self.VECTOR_EXTENSIONS = ['.shp', '.geojson', '.json', '.gpkg', '.parquet', '.geoparquet', '.kml', '.gml']\n",
    "        self.RASTER_EXTENSIONS = ['.tif', '.tiff', '.img', '.jp2', '.png', '.jpg', '.jpeg', '.bmp', '.gif']\n",
    "        \n",
    "        # Parametri processing\n",
    "        self.MIN_VALID_HEIGHT = 3.0    # Altezza minima valida (m)\n",
    "        self.MAX_SUBMERSION_PERCENT = 100.0  # Cap percentuale sommersione\n",
    "        self.PROGRESS_INTERVAL = 100   # Ogni quanti edifici mostrare progresso\n",
    "        \n",
    "        # File management\n",
    "        self.SUPPORTED_FILE_TYPES = {\n",
    "            'vettoriale': self.VECTOR_EXTENSIONS,\n",
    "            'raster': self.RASTER_EXTENSIONS\n",
    "        }\n",
    "        \n",
    "        # Metadati sistema\n",
    "        self.ELAB_ID = None\n",
    "        self.EVENT_NAME = None\n",
    "        self.INPUT_VECTOR_FILE = None\n",
    "        self.INPUT_RASTER_FILE = None\n",
    "        self.OUTPUT_FOLDER = \"output_inondazioni\"\n",
    "        \n",
    "        # Sistema logging e output\n",
    "        self.ENABLE_LOGGING = True\n",
    "        self.CREATE_REPORT = True\n",
    "        self.CREATE_SHAPEFILE = True\n",
    "        \n",
    "        # Parametri naming personalizzato\n",
    "        self.OUTPUT_DATASET_NAME = None      # Nome dataset output personalizzato\n",
    "        self.OUTPUT_FOLDER_NAME = None       # Nome folder output personalizzato  \n",
    "        self.OUTPUT_FILE_PREFIX = None       # Prefisso file output\n",
    "        self.OUTPUT_FILE_SUFFIX = None       # Suffisso file output\n",
    "        self.INCLUDE_TIMESTAMP = True        # Include timestamp nei nomi\n",
    "        \n",
    "        # Dataiku integration\n",
    "        self._dataiku_available = False\n",
    "        self._dataiku = None\n",
    "        try:\n",
    "            import dataiku\n",
    "            self._dataiku_available = True\n",
    "            self._dataiku = dataiku\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        # Carica da payload se fornito\n",
    "        if payload:\n",
    "            self._prepare_scenario_payload(payload)\n",
    "    \n",
    "    def _prepare_scenario_payload(self, payload: dict):\n",
    "        \"\"\"Carica parametri dal payload implementando gerarchia di priorit√†:\n",
    "        1. Scenario JSON (payload) - priorit√† pi√π alta\n",
    "        2. Dataset Dataiku - priorit√† media  \n",
    "        3. Valori di default - priorit√† pi√π bassa\n",
    "        \n",
    "        N.B. Parsing CASE-INSENSITIVE per maggiore flessibilit√†\n",
    "        \"\"\"\n",
    "        # Tracciamento per warning system\n",
    "        self._json_parameters_used = []\n",
    "        self._json_parameters_ignored = []\n",
    "        self._dataiku_fallback_used = False\n",
    "        \n",
    "        # Mapping principale con tutte le variazioni case-insensitive\n",
    "        mapping = {\n",
    "            \"HEIGHT_FIELD\": \"HEIGHT_FIELD\",\n",
    "            \"REPROJECTION_OPTION\": \"REPROJECTION_OPTION\", \n",
    "            \"TARGET_EPSG\": \"TARGET_EPSG\",\n",
    "            \"BUFFER_DISTANCE\": \"BUFFER_DISTANCE\",\n",
    "            \"elab_id\": \"ELAB_ID\",\n",
    "            \"event_name\": \"EVENT_NAME\",\n",
    "            \"min_valid_height\": \"MIN_VALID_HEIGHT\",\n",
    "            \"enable_logging\": \"ENABLE_LOGGING\",\n",
    "            \"create_report\": \"CREATE_REPORT\",\n",
    "            \"create_shapefile\": \"CREATE_SHAPEFILE\"\n",
    "        }\n",
    "        \n",
    "        # Crea un mapping case-insensitive: lowercase_key -> original_key\n",
    "        case_insensitive_mapping = {}\n",
    "        for key in mapping.keys():\n",
    "            case_insensitive_mapping[key.lower()] = key\n",
    "        \n",
    "        # Crea dizionario payload normalizzato (lowercase keys)\n",
    "        payload_normalized = {k.lower(): v for k, v in payload.items()}\n",
    "        \n",
    "        # Rileva parametri non riconosciuti (case-insensitive)\n",
    "        reserved_keys = set(['files', 'output_naming'])\n",
    "        for original_key in payload.keys():\n",
    "            key_lower = original_key.lower()\n",
    "            if (key_lower not in case_insensitive_mapping and \n",
    "                key_lower not in reserved_keys):\n",
    "                self._json_parameters_ignored.append(original_key)\n",
    "        \n",
    "        # FASE 1: Carica parametri dal JSON scenario (priorit√† pi√π alta)\n",
    "        for canonical_key, attr in mapping.items():\n",
    "            key_lower = canonical_key.lower()\n",
    "            if key_lower in payload_normalized:\n",
    "                val = payload_normalized[key_lower]\n",
    "                self._json_parameters_used.append(canonical_key)  # Usa nome canonico per reporting\n",
    "                \n",
    "                # Conversioni specifiche\n",
    "                if attr == \"REPROJECTION_OPTION\":\n",
    "                    setattr(self, attr, int(val))\n",
    "                elif attr == \"BUFFER_DISTANCE\":\n",
    "                    if str(val).lower() == \"auto\":\n",
    "                        setattr(self, attr, None)\n",
    "                        # Marca come esplicitamente impostato da JSON per evitare override\n",
    "                        setattr(self, '_buffer_distance_from_json', True) \n",
    "                    else:\n",
    "                        setattr(self, attr, float(val))\n",
    "                        setattr(self, '_buffer_distance_from_json', True)\n",
    "                elif attr in (\"ENABLE_LOGGING\", \"CREATE_REPORT\", \"CREATE_SHAPEFILE\"):\n",
    "                    setattr(self, attr, self._to_bool(val))\n",
    "                else:\n",
    "                    setattr(self, attr, val)\n",
    "        \n",
    "        # FASE 2: Integra con dataset Dataiku per parametri mancanti (priorit√† media)\n",
    "        if self._dataiku_available:\n",
    "            self._integrate_with_dataiku_datasets()\n",
    "        \n",
    "        # FASE 3: Estrai file da payload (case-insensitive)\n",
    "        files = self._get_files_section_case_insensitive(payload)\n",
    "        self.INPUT_VECTOR_FILE = files.get('vettoriale')\n",
    "        self.INPUT_RASTER_FILE = files.get('raster')\n",
    "        \n",
    "        # FASE 4: Gestisci parametri di naming personalizzato (case-insensitive)\n",
    "        self._load_output_naming_case_insensitive(payload)\n",
    "    \n",
    "    def _integrate_with_dataiku_datasets(self):\n",
    "        \"\"\"Integra configurazione con dataset Dataiku per parametri mancanti\"\"\"\n",
    "        dataiku_parameters_used = []\n",
    "        \n",
    "        try:\n",
    "            # Carica parametri da dataset configurazione_parametri\n",
    "            conf_params = self._dataiku.Dataset(\"configurazione_parametri\").get_dataframe()\n",
    "            \n",
    "            # Mappa parametri dai dataset solo se non sono gi√† stati impostati dal JSON\n",
    "            param_mapping = {\n",
    "                \"HEIGHT_FIELD\": \"HEIGHT_FIELD\",\n",
    "                \"REPROJECTION_OPTION\": \"REPROJECTION_OPTION\",\n",
    "                \"TARGET_EPSG\": \"TARGET_EPSG\",\n",
    "                \"BUFFER_DISTANCE\": \"BUFFER_DISTANCE\",\n",
    "                \"EVENT_NAME\": \"event_name\",\n",
    "                \"MIN_VALID_HEIGHT\": \"min_valid_height\"\n",
    "            }\n",
    "            \n",
    "            for _, row in conf_params.iterrows():\n",
    "                var_name = row['variabile']\n",
    "                var_value = row['valore']\n",
    "                \n",
    "                # Mappa solo se il parametro corrispondente non √® gi√† impostato\n",
    "                if var_name in param_mapping:\n",
    "                    attr_name = param_mapping[var_name]\n",
    "                    current_value = getattr(self, attr_name)\n",
    "                    \n",
    "                    # Applica solo se il valore corrente √® None o valore di default\n",
    "                    if self._should_override_with_dataiku(attr_name, current_value):\n",
    "                        dataiku_parameters_used.append(var_name)\n",
    "                        if attr_name == \"REPROJECTION_OPTION\":\n",
    "                            setattr(self, attr_name, int(var_value))\n",
    "                        elif attr_name == \"BUFFER_DISTANCE\":\n",
    "                            if str(var_value).lower() == \"auto\":\n",
    "                                setattr(self, attr_name, None)\n",
    "                            else:\n",
    "                                setattr(self, attr_name, float(var_value))\n",
    "                        else:\n",
    "                            setattr(self, attr_name, var_value)\n",
    "            \n",
    "            # Traccia se √® stato utilizzato il fallback Dataiku                \n",
    "            if dataiku_parameters_used:\n",
    "                self._dataiku_fallback_used = True\n",
    "                self._dataiku_parameters_used = dataiku_parameters_used\n",
    "                            \n",
    "        except Exception as e:\n",
    "            # Se non riesce a caricare i dataset, continua con i valori attuali\n",
    "            pass\n",
    "    \n",
    "    def _should_override_with_dataiku(self, attr_name: str, current_value):\n",
    "        \"\"\"Determina se un valore dovrebbe essere sovrascritto dai dataset Dataiku\"\"\"\n",
    "        # Controllo speciale per BUFFER_DISTANCE se impostato da JSON\n",
    "        if attr_name == \"BUFFER_DISTANCE\" and hasattr(self, '_buffer_distance_from_json'):\n",
    "            return False  # Non sovrascrivere se √® stato impostato esplicitamente da JSON\n",
    "            \n",
    "        # Valori di default che possono essere sovrascritti\n",
    "        default_values = {\n",
    "            \"HEIGHT_FIELD\": None,\n",
    "            \"REPROJECTION_OPTION\": None,\n",
    "            \"TARGET_EPSG\": None,\n",
    "            \"BUFFER_DISTANCE\": None,\n",
    "            \"EVENT_NAME\": None,\n",
    "            \"MIN_VALID_HEIGHT\": 3.0\n",
    "        }\n",
    "        \n",
    "        # Sovrascrivi solo se il valore corrente √® None o √® il valore di default\n",
    "        return current_value is None or current_value == default_values.get(attr_name)\n",
    "    \n",
    "    def _get_files_section_case_insensitive(self, payload: dict):\n",
    "        \"\"\"Estrae la sezione files dal payload in modo case-insensitive\"\"\"\n",
    "        files_section = {}\n",
    "        \n",
    "        # Cerca la sezione 'files' con variazioni di case\n",
    "        for key, value in payload.items():\n",
    "            if key.lower() == 'files' and isinstance(value, dict):\n",
    "                # Normalizza anche le chiavi interne (vettoriale/raster)\n",
    "                for file_key, file_value in value.items():\n",
    "                    file_key_lower = file_key.lower()\n",
    "                    if file_key_lower in ['vettoriale', 'vector', 'shapefile']:\n",
    "                        files_section['vettoriale'] = file_value\n",
    "                    elif file_key_lower in ['raster', 'tiff', 'geotiff']:\n",
    "                        files_section['raster'] = file_value\n",
    "                break\n",
    "        \n",
    "        return files_section\n",
    "    \n",
    "    def _load_output_naming_case_insensitive(self, payload: dict):\n",
    "        \"\"\"Carica parametri di naming personalizzato dall'output_naming del payload\"\"\"\n",
    "        output_naming = {}\n",
    "        \n",
    "        # Cerca la sezione 'output_naming' con variazioni di case\n",
    "        for key, value in payload.items():\n",
    "            if key.lower() in ['output_naming', 'outputnaming', 'naming'] and isinstance(value, dict):\n",
    "                output_naming = value\n",
    "                break\n",
    "        \n",
    "        if output_naming:\n",
    "            # Mapping con supporto case-insensitive\n",
    "            naming_mapping = {\n",
    "                'dataset_name': 'OUTPUT_DATASET_NAME',\n",
    "                'folder_name': 'OUTPUT_FOLDER_NAME', \n",
    "                'file_prefix': 'OUTPUT_FILE_PREFIX',\n",
    "                'file_suffix': 'OUTPUT_FILE_SUFFIX',\n",
    "                'include_timestamp': 'INCLUDE_TIMESTAMP'\n",
    "            }\n",
    "            \n",
    "            # Normalizza chiavi output_naming\n",
    "            output_naming_normalized = {k.lower(): v for k, v in output_naming.items()}\n",
    "            \n",
    "            for json_key, attr_name in naming_mapping.items():\n",
    "                if json_key in output_naming_normalized:\n",
    "                    val = output_naming_normalized[json_key]\n",
    "                    if attr_name == 'INCLUDE_TIMESTAMP':\n",
    "                        setattr(self, attr_name, self._to_bool(val))\n",
    "                    else:\n",
    "                        setattr(self, attr_name, val)\n",
    "    \n",
    "    def _to_bool(self, val):\n",
    "        \"\"\"Converte valori in booleano in modo sicuro\"\"\"\n",
    "        if isinstance(val, bool):\n",
    "            return val\n",
    "        if isinstance(val, (int, float)):\n",
    "            return bool(val)\n",
    "        if isinstance(val, str):\n",
    "            return val.lower() in ('true', '1', 'yes', 'on')\n",
    "        return False\n",
    "    \n",
    "    def get_configuration_warnings(self):\n",
    "        \"\"\"Restituisce una lista di warning per la configurazione JSON\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        # Warning per parametri JSON ignorati\n",
    "        if hasattr(self, '_json_parameters_ignored') and self._json_parameters_ignored:\n",
    "            warnings.append({\n",
    "                'type': 'IGNORED_JSON_PARAMETERS',\n",
    "                'message': f\"Parametri JSON non riconosciuti (ignorati): {', '.join(self._json_parameters_ignored)}\",\n",
    "                'details': 'Questi parametri nel JSON scenario sono stati ignorati perch√© non corrispondono ai nomi di parametri supportati',\n",
    "                'suggestion': 'Verifica la spelling dei parametri JSON, i nomi devono essere corretti'\n",
    "            })\n",
    "        \n",
    "        # Warning per fallback su dataset Dataiku\n",
    "        if hasattr(self, '_dataiku_fallback_used') and self._dataiku_fallback_used:\n",
    "            warnings.append({\n",
    "                'type': 'DATAIKU_FALLBACK_USED',\n",
    "                'message': f\"Parametri caricati dai dataset Dataiku: {', '.join(getattr(self, '_dataiku_parameters_used', []))}\",\n",
    "                'details': 'Alcuni parametri critici non erano presenti nel JSON scenario, il sistema ha fatto fallback sui dataset Dataiku',\n",
    "                'suggestion': 'Per controllo completo, includi tutti i parametri nel JSON scenario'\n",
    "            })\n",
    "        \n",
    "        # Warning per parametri critici mancanti \n",
    "        critical_params = ['HEIGHT_FIELD', 'TARGET_EPSG', 'REPROJECTION_OPTION']\n",
    "        missing_critical = []\n",
    "        for param in critical_params:\n",
    "            if getattr(self, param, None) is None:\n",
    "                missing_critical.append(param)\n",
    "        \n",
    "        if missing_critical:\n",
    "            warnings.append({\n",
    "                'type': 'MISSING_CRITICAL_PARAMETERS',\n",
    "                'message': f\"Parametri critici non configurati: {', '.join(missing_critical)}\",\n",
    "                'details': 'Questi parametri sono essenziali per il funzionamento corretto dell\\'analisi',\n",
    "                'suggestion': 'Configura questi parametri nei dataset Dataiku o nel JSON scenario'\n",
    "            })\n",
    "        \n",
    "        return warnings\n",
    "    \n",
    "    def get_output_names(self):\n",
    "        \"\"\"Genera nomi di output personalizzati basati sui parametri di naming\"\"\"\n",
    "        from datetime import datetime\n",
    "        \n",
    "        # Timestamp base\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if self.INCLUDE_TIMESTAMP else \"\"\n",
    "        \n",
    "        # Nome dataset output\n",
    "        if self.OUTPUT_DATASET_NAME:\n",
    "            dataset_name = self.OUTPUT_DATASET_NAME\n",
    "            if self.INCLUDE_TIMESTAMP and timestamp:\n",
    "                dataset_name = f\"{dataset_name}_{timestamp}\"\n",
    "        else:\n",
    "            # Nome default con eventuale timestamp\n",
    "            dataset_name = f\"output_inondazioni_{timestamp}\" if timestamp else \"output_inondazioni\"\n",
    "        \n",
    "        # Nome folder output  \n",
    "        if self.OUTPUT_FOLDER_NAME:\n",
    "            folder_name = self.OUTPUT_FOLDER_NAME\n",
    "            if self.INCLUDE_TIMESTAMP and timestamp:\n",
    "                folder_name = f\"{folder_name}_{timestamp}\"\n",
    "        else:\n",
    "            # Nome default con eventuale timestamp\n",
    "            folder_name = f\"output_inondazioni_{timestamp}\" if timestamp else \"output_inondazioni\"\n",
    "        \n",
    "        # Pattern per file individuali\n",
    "        file_pattern = \"\"\n",
    "        if self.OUTPUT_FILE_PREFIX:\n",
    "            file_pattern += self.OUTPUT_FILE_PREFIX\n",
    "        if timestamp and self.INCLUDE_TIMESTAMP:\n",
    "            file_pattern += f\"analysis_{timestamp}\"\n",
    "        else:\n",
    "            file_pattern += \"analysis\"\n",
    "        if self.OUTPUT_FILE_SUFFIX:\n",
    "            file_pattern += f\"_{self.OUTPUT_FILE_SUFFIX}\"\n",
    "        \n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'folder_name': folder_name, \n",
    "            'file_pattern': file_pattern,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "    \n",
    "    def validate_config(self):\n",
    "        \"\"\"Valida configurazione e ritorna lista errori\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.HEIGHT_FIELD:\n",
    "            errors.append(\"HEIGHT_FIELD deve essere specificato\")\n",
    "        \n",
    "        if self.REPROJECTION_OPTION not in [1, 2, 3]:\n",
    "            errors.append(\"REPROJECTION_OPTION deve essere 1, 2 o 3\")\n",
    "        \n",
    "        if not self.TARGET_EPSG:\n",
    "            errors.append(\"TARGET_EPSG deve essere specificato\")\n",
    "        \n",
    "        if self.BUFFER_DISTANCE is not None and self.BUFFER_DISTANCE <= 0:\n",
    "            errors.append(\"BUFFER_DISTANCE deve essere > 0 o None (automatico)\")\n",
    "            \n",
    "        return errors\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Visualizza configurazione corrente\"\"\"\n",
    "        print(f\"\\n=== CONFIGURAZIONE FLOOD ANALYSIS ===\")\n",
    "        print(f\"Elaborazione ID: {self.ELAB_ID or 'N/A'}\")\n",
    "        print(f\"Nome evento: {self.EVENT_NAME or 'N/A'}\")\n",
    "        print(f\"Campo altezza: {self.HEIGHT_FIELD}\")\n",
    "        print(f\"Opzione riproiezione: {self.REPROJECTION_OPTION}\")\n",
    "        print(f\"Target EPSG: {self.TARGET_EPSG}\")\n",
    "        print(f\"Buffer distance: {self.BUFFER_DISTANCE or 'automatico'}\")\n",
    "        print(f\"Altezza minima valida: {self.MIN_VALID_HEIGHT}m\")\n",
    "        print(f\"File vettoriale: {self.INPUT_VECTOR_FILE or 'N/A'}\")\n",
    "        print(f\"File raster: {self.INPUT_RASTER_FILE or 'N/A'}\")\n",
    "        print(f\"Output folder: {self.OUTPUT_FOLDER}\")\n",
    "        print(f\"--- Controlli di output ---\")\n",
    "        print(f\"Logging attivo: {self.ENABLE_LOGGING}\")\n",
    "        print(f\"Creazione report: {self.CREATE_REPORT}\")\n",
    "        print(f\"Creazione shapefile: {self.CREATE_SHAPEFILE}\")\n",
    "        \n",
    "        # Mostra naming personalizzato se configurato\n",
    "        if any([self.OUTPUT_DATASET_NAME, self.OUTPUT_FOLDER_NAME, self.OUTPUT_FILE_PREFIX, self.OUTPUT_FILE_SUFFIX]):\n",
    "            print(f\"--- Naming personalizzato ---\")\n",
    "            if self.OUTPUT_DATASET_NAME:\n",
    "                print(f\"Nome dataset custom: {self.OUTPUT_DATASET_NAME}\")\n",
    "            if self.OUTPUT_FOLDER_NAME:\n",
    "                print(f\"Nome folder custom: {self.OUTPUT_FOLDER_NAME}\")\n",
    "            if self.OUTPUT_FILE_PREFIX:\n",
    "                print(f\"Prefisso file: {self.OUTPUT_FILE_PREFIX}\")\n",
    "            if self.OUTPUT_FILE_SUFFIX:\n",
    "                print(f\"Suffisso file: {self.OUTPUT_FILE_SUFFIX}\")\n",
    "            print(f\"Include timestamp: {self.INCLUDE_TIMESTAMP}\")\n",
    "            \n",
    "            # Mostra preview dei nomi generati\n",
    "            output_names = self.get_output_names()\n",
    "            print(f\"üìù Preview nomi output:\")\n",
    "            print(f\"  Dataset: {output_names['dataset_name']}\")\n",
    "            print(f\"  Folder: {output_names['folder_name']}\")\n",
    "            print(f\"  Pattern file: {output_names['file_pattern']}\")\n",
    "        \n",
    "        print(f\"--- Sistema ---\")\n",
    "        print(f\"Dataiku disponibile: {self._dataiku_available}\")\n",
    "        print(f\"Formati vettoriali: {len(self.VECTOR_EXTENSIONS)} supportati\")\n",
    "        print(f\"Formati raster: {len(self.RASTER_EXTENSIONS)} supportati\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "print(\"‚úÖ FloodAnalysisConfig definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Funzione principale per analisi sommersione edifici - Flood Analysis\n",
    "    \"\"\"\n",
    "    print(\"üöÄ AVVIO ANALISI SOMMERSIONE EDIFICI\")\n",
    "    \n",
    "    try:\n",
    "        # =================================================================\n",
    "        # FASE 1: CREAZIONE PAYLOAD\n",
    "        # =================================================================\n",
    "        print(\"=== CREAZIONE PAYLOAD ===\")\n",
    "        \n",
    "        # Rileva automaticamente la modalit√† di esecuzione \n",
    "        payload = _create_payload()\n",
    "        print(f\"üìã Payload ID: {payload.get('elab_id')}\")\n",
    "        \n",
    "        # Crea configurazione dal payload\n",
    "        config = FloodAnalysisConfig(payload)\n",
    "        \n",
    "        # Validazione configurazione \n",
    "        config_errors = config.validate_config()\n",
    "        if config_errors:\n",
    "            error_msg = f\"‚ùå ERRORI CONFIGURAZIONE:\\n\" + \"\\n\".join(f\"  - {err}\" for err in config_errors)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Visualizza configurazione finale\n",
    "        config.print_config()\n",
    "        \n",
    "        # Legge configurazione dati (file da processare)\n",
    "        try:\n",
    "            configurazione_dati = dataiku.Dataset(\"configurazione_dati\")\n",
    "            configurazione_dati_df = configurazione_dati.get_dataframe()\n",
    "            \n",
    "            print(\"\\n=== DATASET CONFIGURAZIONE DATI ===\")\n",
    "            print(f\"Righe nel dataset: {len(configurazione_dati_df)}\")\n",
    "            \n",
    "            # Estrai file configurati\n",
    "            try:\n",
    "                shapefile_config = configurazione_dati_df[configurazione_dati_df['tipo_file'] == 'vettoriale']['nome_file'].iloc[0]\n",
    "                raster_config = configurazione_dati_df[configurazione_dati_df['tipo_file'] == 'raster']['nome_file'].iloc[0]\n",
    "                \n",
    "                config.INPUT_VECTOR_FILE = shapefile_config\n",
    "                config.INPUT_RASTER_FILE = raster_config\n",
    "                \n",
    "            except (IndexError, KeyError):\n",
    "                print(\"‚ö†Ô∏è File non trovati nel dataset configurazione_dati\")\n",
    "                print(f\"üí° Usando file da payload: vettoriale={config.INPUT_VECTOR_FILE}, raster={config.INPUT_RASTER_FILE}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Dataset configurazione_dati non disponibile: {e}\")\n",
    "            print(\"üí° Usando configurazione file da payload\")\n",
    "        \n",
    "        print(f\"\\n=== FILE CONFIGURATI FINALI ===\")\n",
    "        print(f\"Shapefile configurato: {config.INPUT_VECTOR_FILE}\")\n",
    "        print(f\"Raster configurato: {config.INPUT_RASTER_FILE}\")\n",
    "        \n",
    "        # Ritorna configurazione completa\n",
    "        return {\n",
    "            'config': config,\n",
    "            'payload': payload,\n",
    "            # Backward compatibility\n",
    "            'HEIGHT_FIELD': config.HEIGHT_FIELD,\n",
    "            'REPROJECTION_OPTION': config.REPROJECTION_OPTION,\n",
    "            'TARGET_EPSG': config.TARGET_EPSG,\n",
    "            'BUFFER_DISTANCE': config.BUFFER_DISTANCE,\n",
    "            'shapefile_config': config.INPUT_VECTOR_FILE,\n",
    "            'raster_config': config.INPUT_RASTER_FILE\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRORE CRITICO nella configurazione: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def _create_payload_from_config_tables():\n",
    "    \"\"\"\n",
    "    Crea payload leggendo dai dataset di configurazione Dataiku\n",
    "    (seguendo il pattern del notebook geospaziale)\n",
    "    \"\"\"\n",
    "    print(\"üìä Creazione payload da tabelle di configurazione Dataiku...\")\n",
    "    \n",
    "    payload = {\n",
    "        \"elab_id\": f\"flood_{datetime.now(pytz.timezone('Europe/Rome')).strftime('%Y%m%d_%H%M%S')}\"\n",
    "    }\n",
    "    \n",
    "    # Leggi parametri da configurazione_parametri\n",
    "    try:\n",
    "        conf_params = dataiku.Dataset(\"configurazione_parametri\").get_dataframe()\n",
    "        for _, row in conf_params.iterrows():\n",
    "            payload[row['variabile']] = row['valore']\n",
    "        print(f\"‚úÖ Parametri letti: {list(payload.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore lettura configurazione_parametri: {e}\")\n",
    "    \n",
    "    # Leggi file da configurazione_dati\n",
    "    try:\n",
    "        conf_data = dataiku.Dataset(\"configurazione_dati\").get_dataframe()\n",
    "        payload['files'] = {}\n",
    "        for _, row in conf_data.iterrows():\n",
    "            payload['files'][row['tipo_file']] = row['nome_file']\n",
    "        print(f\"‚úÖ File configurati: {list(payload.get('files', {}).keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore lettura configurazione_dati: {e}\")\n",
    "        payload['files'] = {}\n",
    "    \n",
    "    return payload\n",
    "\n",
    "def _create_payload():\n",
    "    \"\"\"\n",
    "    Crea payload per la configurazione del sistema:\n",
    "    - Se lanciato da scenario ‚Üí usa parametri scenario  \n",
    "    - Se lanciato manualmente ‚Üí legge da dataset di configurazione\n",
    "    \"\"\"\n",
    "    payload = None\n",
    "    \n",
    "    try:\n",
    "        # Rileva se lanciato da scenario\n",
    "        run_vars = dataiku.get_custom_variables()\n",
    "        scenario_run_id = run_vars.get('scenarioTriggerRunId')\n",
    "        \n",
    "        if scenario_run_id is not None:\n",
    "            # ü§ñ AUTOMAZIONE: Scenario trigger\n",
    "            print(f\"ü§ñ Rilevata esecuzione da scenario: {scenario_run_id}\")\n",
    "            scenario_params = run_vars.get('scenarioTriggerParams')\n",
    "            if scenario_params:\n",
    "                payload = json.loads(scenario_params)\n",
    "                if payload.get('elab_id') is None:\n",
    "                    payload[\"elab_id\"] = f\"flood_{scenario_run_id.replace('-', '')[:-3]}\"\n",
    "                print(\"‚úÖ Payload da scenario trigger caricato\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Scenario trigger senza parametri, fallback su dataset\")\n",
    "                payload = _create_payload_from_config_tables()\n",
    "        else:\n",
    "            # üë§ MANUALE: Esecuzione da flow\n",
    "            print(\"üë§ Rilevata esecuzione manuale da flow\")\n",
    "            payload = _create_payload_from_config_tables()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore rilevamento modalit√† esecuzione: {e}\")\n",
    "        print(\"üí° Fallback su configurazione da dataset\")\n",
    "        payload = _create_payload_from_config_tables()\n",
    "    \n",
    "    return payload or {}\n",
    "\n",
    "print(\"‚úÖ Funzione main() definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESECUZIONE MAIN - ANALISI SOMMERSIONE EDIFICI\n",
    "print(\"üéØ AVVIO PROCEDURA PRINCIPALE\")\n",
    "\n",
    "# Esegui funzione principale e ottieni configurazione\n",
    "config_result = main()\n",
    "\n",
    "# Estrai configurazione\n",
    "flood_config = config_result['config']\n",
    "flood_payload = config_result['payload']\n",
    "\n",
    "# Inizializza error handler\n",
    "error_handler = ErrorHandler(flood_config)\n",
    "\n",
    "# Backward compatibility - estrai parametri singoli\n",
    "HEIGHT_FIELD = flood_config.HEIGHT_FIELD\n",
    "REPROJECTION_OPTION = flood_config.REPROJECTION_OPTION  \n",
    "TARGET_EPSG = flood_config.TARGET_EPSG\n",
    "BUFFER_DISTANCE = flood_config.BUFFER_DISTANCE\n",
    "shapefile_config = flood_config.INPUT_VECTOR_FILE\n",
    "raster_config = flood_config.INPUT_RASTER_FILE\n",
    "\n",
    "print(f\"\\n‚úÖ CONFIGURAZIONE COMPLETATA\")\n",
    "print(f\"üìã Payload ID: {flood_payload.get('elab_id')}\")\n",
    "print(f\"üîß Error Handler inizializzato\")\n",
    "print(f\"‚öôÔ∏è  FloodAnalysisConfig pronta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b89e3",
   "metadata": {},
   "source": [
    "## 2. Carica Dati di Input da Dataiku\n",
    "\n",
    "Lettura dei parametri di configurazione dal dataset e accesso ai file vettoriali e raster dal folder Minio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1334c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesso al folder di input contenente i file di analisi\n",
    "minio_input = dataiku.Folder(\"minio_input\")\n",
    "\n",
    "print(\"=== INFORMAZIONI FOLDER INPUT ===\")\n",
    "print(f\"Folder minio_input: {minio_input.get_info()}\")\n",
    "\n",
    "# Elenco dei file disponibili nel folder\n",
    "input_files = minio_input.list_paths_in_partition()\n",
    "\n",
    "print(f\"\\nFile disponibili nel folder minio_input:\")\n",
    "for file_path in input_files:\n",
    "    print(f\"  - {file_path}\")\n",
    "\n",
    "# DEFINIZIONE FORMATI SUPPORTATI\n",
    "# Formati vettoriali supportati da GeoPandas/Fiona\n",
    "VECTOR_EXTENSIONS = ['.shp', '.geojson', '.json', '.gpkg', '.parquet', '.geoparquet', '.kml', '.gml']\n",
    "# Formati raster supportati da Rasterio/GDAL  \n",
    "RASTER_EXTENSIONS = ['.tif', '.tiff', '.img', '.jp2', '.png', '.jpg', '.jpeg', '.bmp', '.gif']\n",
    "\n",
    "# Classificazione dei file per tipologia con supporto multi-formato\n",
    "vector_files = []\n",
    "raster_files = []\n",
    "\n",
    "for file_path in input_files:\n",
    "    file_lower = file_path.lower()\n",
    "    \n",
    "    # Controlla formati vettoriali\n",
    "    if any(file_lower.endswith(ext) for ext in VECTOR_EXTENSIONS):\n",
    "        vector_files.append(file_path)\n",
    "    \n",
    "    # Controlla formati raster\n",
    "    elif any(file_lower.endswith(ext) for ext in RASTER_EXTENSIONS):\n",
    "        raster_files.append(file_path)\n",
    "\n",
    "print(f\"\\n=== RIEPILOGO FILE PER TIPOLOGIA ===\")\n",
    "print(f\"File vettoriali trovati: {len(vector_files)}\")\n",
    "for vec in vector_files:\n",
    "    file_ext = '.' + vec.split('.')[-1].upper()\n",
    "    print(f\"  - {vec} [{file_ext}]\")\n",
    "\n",
    "print(f\"File raster trovati: {len(raster_files)}\")\n",
    "for ras in raster_files:\n",
    "    file_ext = '.' + ras.split('.')[-1].upper()\n",
    "    print(f\"  - {ras} [{file_ext}]\")\n",
    "\n",
    "# Verifica presenza dei file necessari\n",
    "if len(vector_files) == 0:\n",
    "    supported_vec = ', '.join(VECTOR_EXTENSIONS)\n",
    "    raise ValueError(f\"‚ùå Nessun file vettoriale trovato nel folder minio_input!\\n\"\n",
    "                    f\"Formati supportati: {supported_vec}\")\n",
    "                    \n",
    "if len(raster_files) == 0:\n",
    "    supported_ras = ', '.join(RASTER_EXTENSIONS)\n",
    "    raise ValueError(f\"‚ùå Nessun file raster trovato nel folder minio_input!\\n\"\n",
    "                    f\"Formati supportati: {supported_ras}\")\n",
    "\n",
    "print(f\"\\nüìã Formati vettoriali supportati: {', '.join(VECTOR_EXTENSIONS)}\")\n",
    "print(f\"üìã Formati raster supportati: {', '.join(RASTER_EXTENSIONS[:8])}... (+{len(RASTER_EXTENSIONS)-8} altri)\")\n",
    "\n",
    "# SELEZIONE INTELLIGENTE MIGLIORATA CON PAYLOAD\n",
    "def find_configured_file(file_list, configured_path, file_type):\n",
    "    \"\"\"Trova il file che corrisponde alla configurazione con percorso completo\"\"\"\n",
    "    if not configured_path:\n",
    "        if file_list:\n",
    "            print(f\"‚ö†Ô∏è Nessun {file_type} configurato, uso il primo disponibile: {file_list[0]}\")\n",
    "            return file_list[0]\n",
    "        return None\n",
    "    \n",
    "    configured_filename = configured_path.split('/')[-1]\n",
    "    \n",
    "    # 1. Match esatto per nome file\n",
    "    exact_matches = [f for f in file_list if configured_filename in f]\n",
    "    if exact_matches:\n",
    "        print(f\"‚úì Trovato {file_type} configurato '{configured_filename}': {exact_matches[0]}\")\n",
    "        return exact_matches[0]\n",
    "    \n",
    "    # 2. Match parziale nel percorso\n",
    "    path_matches = [f for f in file_list if configured_path.replace('/', '\\\\') in f or configured_path.replace('\\\\', '/') in f]\n",
    "    if path_matches:\n",
    "        print(f\"‚úì Trovato {file_type} con percorso '{configured_path}': {path_matches[0]}\")\n",
    "        return path_matches[0]\n",
    "    \n",
    "    # 3. Fallback robusto\n",
    "    if file_list:\n",
    "        print(f\"‚ö†Ô∏è {file_type} configurato '{configured_path}' non trovato, uso il primo disponibile: {file_list[0]}\")\n",
    "        print(f\"   - File cercato: {configured_filename}\")\n",
    "        return file_list[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(f\"\\n=== SELEZIONE BASATA SU CONFIGURAZIONE ===\")\n",
    "print(f\"üìÅ File vettoriale configurato: {shapefile_config}\")\n",
    "print(f\"üìÅ File raster configurato: {raster_config}\")\n",
    "\n",
    "# Seleziona i file basandosi sulla configurazione (ora con nomi generici)\n",
    "vector_file = find_configured_file(vector_files, shapefile_config, \"file vettoriale\")\n",
    "raster_file = find_configured_file(raster_files, raster_config, \"file raster\")\n",
    "\n",
    "print(f\"\\n=== FILE SELEZIONATI PER L'ANALISI ===\\n\")\n",
    "print(f\"üìÑ File vettoriale: {vector_file}\")\n",
    "print(f\"üó∫Ô∏è  File raster: {raster_file}\")\n",
    "\n",
    "# Mostra file alternativi disponibili\n",
    "if len(vector_files) > 1:\n",
    "    print(f\"\\nüìã Altri file vettoriali disponibili:\")\n",
    "    for vec in vector_files:\n",
    "        if vec != vector_file:\n",
    "            file_ext = '.' + vec.split('.')[-1].upper()\n",
    "            print(f\"   - {vec} [{file_ext}]\")\n",
    "\n",
    "if len(raster_files) > 1:\n",
    "    print(f\"\\nüìã Altri file raster disponibili:\")\n",
    "    for ras in raster_files:\n",
    "        if ras != raster_file:\n",
    "            file_ext = '.' + ras.split('.')[-1].upper()\n",
    "            print(f\"   - {ras} [{file_ext}]\")\n",
    "\n",
    "print(f\"\\nüí° Per cambiare selezione, modifica il dataset 'configurazione_dati'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3dfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dei file dal folder di input verso directory temporanea locale\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Directory temporanea creata: {temp_dir}\")\n",
    "\n",
    "# Download con funzione robusta migliorata\n",
    "print(f\"üì• Download in corso: {vector_file}\")\n",
    "try:\n",
    "    vector_local_path = _download_remote_to_tmp(vector_file, minio_input, temp_dir)\n",
    "    if vector_local_path:\n",
    "        print(f\"‚úÖ Vector file scaricato: {os.path.basename(vector_local_path)}\")\n",
    "    else:\n",
    "        raise Exception(f\"Download fallito per {vector_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore download vector: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"üì• Download in corso: {raster_file}\")\n",
    "try:\n",
    "    raster_local_path = _download_remote_to_tmp(raster_file, minio_input, temp_dir)\n",
    "    if raster_local_path:\n",
    "        print(f\"‚úÖ Raster file scaricato: {os.path.basename(raster_local_path)}\")\n",
    "    else:\n",
    "        raise Exception(f\"Download fallito per {raster_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore download raster: {e}\")\n",
    "    raise\n",
    "\n",
    "# Informazioni sui file scaricati\n",
    "if vector_local_path.lower().endswith('.shp'):\n",
    "    print(f\"üìÅ Shapefile completo scaricato (con file accessori)\")\n",
    "elif vector_local_path.lower().endswith(('.gpkg', '.gdb')):\n",
    "    print(f\"üìÅ File vettoriale database rilevato - formato autocontenuto\")\n",
    "elif vector_local_path.lower().endswith(('.parquet', '.geoparquet')):\n",
    "    print(f\"üìÅ File GeoParquet rilevato - formato colonnare ottimizzato\")\n",
    "else:\n",
    "    vector_ext = vector_local_path.split('.')[-1].upper()\n",
    "    print(f\"üìÅ File vettoriale {vector_ext} - formato autocontenuto\")\n",
    "\n",
    "# Informazioni sui file raster\n",
    "raster_ext = raster_local_path.split('.')[-1].upper() \n",
    "print(f\"üìÅ File raster {raster_ext} scaricato\")\n",
    "\n",
    "print(f\"\\n=== DOWNLOAD COMPLETATO ===\")\n",
    "print(f\"File vettoriale: {vector_local_path}\")\n",
    "print(f\"File raster: {raster_local_path}\")\n",
    "print(f\"Directory di lavoro: {temp_dir}\")\n",
    "print(f\"Formato vettoriale: {vector_local_path.split('.')[-1].upper()}\")\n",
    "print(f\"Formato raster: {raster_local_path.split('.')[-1].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica dati vettoriali e raster con geopandas e rasterio\n",
    "vector = gpd.read_file(vector_local_path)\n",
    "raster = rasterio.open(raster_local_path)\n",
    "\n",
    "print(\"=== DATI CARICATI ===\")\n",
    "print(f\"Edifici nel vettoriale: {len(vector)}\")\n",
    "print(f\"Dimensioni raster: {raster.width} x {raster.height}\")\n",
    "print(f\"CRS vettoriale: {vector.crs}\")\n",
    "print(f\"CRS raster: {raster.crs}\")\n",
    "\n",
    "# Controlla i campi disponibili nel vettoriale\n",
    "print(f\"\\nCampi disponibili nel vettoriale:\")\n",
    "print(list(vector.columns))\n",
    "\n",
    "# Verifica che il campo altezza sia presente\n",
    "if HEIGHT_FIELD not in vector.columns:\n",
    "    raise ValueError(f\"Campo altezza '{HEIGHT_FIELD}' non trovato nel vettoriale! Campi disponibili: {list(vector.columns)}\")\n",
    "    \n",
    "print(f\"\\n‚úì Campo altezza '{HEIGHT_FIELD}' trovato nel vettoriale\")\n",
    "\n",
    "# Rilevamento campo FID (case-insensitive)\n",
    "FID_FIELD = None\n",
    "fid_value_source = None\n",
    "for col in vector.columns:\n",
    "    if col.upper() == 'FID':\n",
    "        FID_FIELD = col\n",
    "        fid_value_source = 'input'  # Eredita dall'input\n",
    "        break\n",
    "\n",
    "if FID_FIELD:\n",
    "    print(f\"‚úì Campo FID trovato nell'input: '{FID_FIELD}' - sar√† ereditato\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Campo FID non presente nell'input - sar√† generato automaticamente\")\n",
    "    fid_value_source = 'generated'  # Genera automaticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dbba7",
   "metadata": {},
   "source": [
    "## 3. Allineamento Sistemi di Riferimento\n",
    "\n",
    "Controllo della compatibilit√† CRS tra dati vettoriali e raster e implementazione della logica di riproiezione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c71d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controllo CRS\n",
    "vector_crs = vector.crs\n",
    "raster_crs = raster.crs\n",
    "\n",
    "if vector_crs != raster_crs:\n",
    "    print(f\"‚ö†Ô∏è  ATTENZIONE: I sistemi di riferimento non coincidono!\")\n",
    "    print(f\"CRS vettoriale: {vector_crs}\")\n",
    "    print(f\"CRS raster: {raster_crs}\")\n",
    "    print(f\"Applicando opzione di riproiezione: {REPROJECTION_OPTION}\")\n",
    "    \n",
    "    try:\n",
    "        if REPROJECTION_OPTION == 1:\n",
    "            # Riproietta vettoriale nel CRS del raster\n",
    "            target_crs = raster_crs\n",
    "            print(f\"Riproiettando il vettoriale in {target_crs}...\")\n",
    "            vector = vector.to_crs(target_crs)\n",
    "            print(\"‚úì Vettoriale riproiettato.\")\n",
    "            \n",
    "        elif REPROJECTION_OPTION == 2:\n",
    "            # Riproietta raster nel CRS del vettoriale\n",
    "            target_crs = vector_crs\n",
    "            print(f\"Riproiettando il raster in {target_crs}...\")\n",
    "            # Crea file temporaneo per raster riproiettato\n",
    "            temp_raster = tempfile.NamedTemporaryFile(suffix='.tif', delete=False)\n",
    "            temp_raster_path = temp_raster.name\n",
    "            temp_raster.close()\n",
    "            \n",
    "            # Calcola trasformazione\n",
    "            transform, width, height = calculate_default_transform(\n",
    "                raster.crs, target_crs, raster.width, raster.height, *raster.bounds)\n",
    "            \n",
    "            # Parametri per il nuovo raster\n",
    "            kwargs = raster.meta.copy()\n",
    "            kwargs.update({\n",
    "                'crs': target_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "            \n",
    "            # Esegui riproiezione\n",
    "            with rasterio.open(temp_raster_path, 'w', **kwargs) as dst:\n",
    "                for i in range(1, raster.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(raster, i),\n",
    "                        destination=rasterio.band(dst, i),\n",
    "                        src_transform=raster.transform,\n",
    "                        src_crs=raster.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=target_crs,\n",
    "                        resampling=Resampling.bilinear)\n",
    "            \n",
    "            # Chiudi raster originale e apri quello riproiettato\n",
    "            raster.close()\n",
    "            raster = rasterio.open(temp_raster_path)\n",
    "            print(\"‚úì Raster riproiettato.\")\n",
    "            \n",
    "        elif REPROJECTION_OPTION == 3:\n",
    "            # Riproietta entrambi nel CRS specificato\n",
    "            target_crs = f\"EPSG:{TARGET_EPSG}\"\n",
    "            print(f\"Riproiettando entrambi in {target_crs}...\")\n",
    "            \n",
    "            # Riproietta vettoriale\n",
    "            vector = vector.to_crs(target_crs)\n",
    "            print(\"‚úì Vettoriale riproiettato.\")\n",
    "            \n",
    "            # Riproietta raster (stesso codice dell'opzione 2)\n",
    "            temp_raster = tempfile.NamedTemporaryFile(suffix='.tif', delete=False)\n",
    "            temp_raster_path = temp_raster.name\n",
    "            temp_raster.close()\n",
    "            \n",
    "            transform, width, height = calculate_default_transform(\n",
    "                raster.crs, target_crs, raster.width, raster.height, *raster.bounds)\n",
    "            \n",
    "            kwargs = raster.meta.copy()\n",
    "            kwargs.update({\n",
    "                'crs': target_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "            \n",
    "            with rasterio.open(temp_raster_path, 'w', **kwargs) as dst:\n",
    "                for i in range(1, raster.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(raster, i),\n",
    "                        destination=rasterio.band(dst, i),\n",
    "                        src_transform=raster.transform,\n",
    "                        src_crs=raster.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=target_crs,\n",
    "                        resampling=Resampling.bilinear)\n",
    "            \n",
    "            raster.close()\n",
    "            raster = rasterio.open(temp_raster_path)\n",
    "            print(\"‚úì Raster riproiettato.\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Opzione di riproiezione non valida: {REPROJECTION_OPTION}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Errore durante la riproiezione: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úì Sistemi di riferimento gi√† compatibili - nessuna riproiezione necessaria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a217d",
   "metadata": {},
   "source": [
    "## 4. Funzioni di Analisi della Profondit√† dell'Acqua\n",
    "\n",
    "Implementazione della funzione `get_external_pixels()` per estrarre i valori di profondit√† dell'acqua dai pixel immediatamente esterni al perimetro degli edifici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_external_pixels(geom, raster, buffer_distance=None):\n",
    "    \"\"\"\n",
    "    Estrae i valori dei pixel immediatamente esterni al perimetro del poligono\n",
    "    \n",
    "    Parametri:\n",
    "    - geom: geometria del poligono (edificio)  \n",
    "    - raster: rasterio dataset con profondit√† acqua\n",
    "    - buffer_distance: distanza buffer in metri (None = automatico = risoluzione pixel)\n",
    "    \n",
    "    Ritorna:\n",
    "    - numpy array con valori di profondit√† validi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Disabilita temporaneamente tutti i warning per evitare messaggi di sovrapposizione\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            # Se non specificato, usa la risoluzione del raster come buffer\n",
    "            if buffer_distance is None:\n",
    "                buffer_distance = abs(raster.transform[0])  # risoluzione pixel\n",
    "            \n",
    "            # Crea buffer esterno molto piccolo\n",
    "            external_buffer = geom.buffer(buffer_distance)\n",
    "            \n",
    "            # Crea anello: buffer esterno - poligono originale\n",
    "            ring = external_buffer.difference(geom)\n",
    "            \n",
    "            # Estrai valori raster dall'anello - questa chiamata pu√≤ generare il warning\n",
    "            out_image, out_transform = rasterio.mask.mask(raster, [mapping(ring)], crop=True, filled=True)\n",
    "            data = out_image[0]\n",
    "            \n",
    "            # Escludi nodata\n",
    "            valid_data = data[data != raster.nodata]\n",
    "            \n",
    "            return valid_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Gestione silenziosa degli errori comuni (es. nessuna sovrapposizione)\n",
    "        # Gli errori saranno tracciati nel conteggio generale\n",
    "        return np.array([])\n",
    "\n",
    "print(\"‚úì Funzione get_external_pixels() definita con soppressione warning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2400184",
   "metadata": {},
   "source": [
    "## 5. Calcolo della Sommersione degli Edifici\n",
    "\n",
    "Elaborazione di ogni edificio per calcolare area, volume e statistiche di sommersione con tracking del progresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40308ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKFLOW MODULARE - FASE PROCESSING EDIFICI\n",
    "def run_flood_analysis_workflow(config: FloodAnalysisConfig, vector, raster, error_handler: ErrorHandler):\n",
    "    \"\"\"\n",
    "    Workflow modulare per analisi sommersione edifici con error handling robusto\n",
    "    \n",
    "    Args:\n",
    "        config: Configurazione FloodAnalysisConfig\n",
    "        vector: GeoDataFrame edifici \n",
    "        raster: Rasterio dataset profondit√† acqua\n",
    "        error_handler: Gestore errori centralizzato\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results_list, processing_stats)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîß AVVIO WORKFLOW MODULARE - {len(vector)} edifici da processare\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # FASE 1: PREPARAZIONE E VALIDAZIONE\n",
    "    # =================================================================\n",
    "    print(\"=== FASE 1: PREPARAZIONE DATI ===\")\n",
    "    \n",
    "    # Validazione campo altezza\n",
    "    if config.HEIGHT_FIELD not in vector.columns:\n",
    "        error_handler.handle_validation_error(\n",
    "            \"MISSING_HEIGHT_FIELD\", \n",
    "            f\"Campo '{config.HEIGHT_FIELD}' non presente. Disponibili: {list(vector.columns)}\"\n",
    "        )\n",
    "        return [], {}\n",
    "    \n",
    "    # Rilevamento campo FID (case-insensitive) - MIGLIORATO\n",
    "    fid_field = None\n",
    "    fid_value_source = None\n",
    "    \n",
    "    for col in vector.columns:\n",
    "        if col.upper() == 'FID':\n",
    "            fid_field = col\n",
    "            fid_value_source = 'input'\n",
    "            break\n",
    "    \n",
    "    if not fid_field:\n",
    "        fid_value_source = 'generated'\n",
    "        print(\"‚ÑπÔ∏è  Campo FID generato automaticamente\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Campo FID ereditato: '{fid_field}'\")\n",
    "    \n",
    "    # Statistiche pre-processing\n",
    "    total_buildings = len(vector)\n",
    "    valid_heights = (vector[config.HEIGHT_FIELD] > config.MIN_VALID_HEIGHT).sum()\n",
    "    invalid_heights = total_buildings - valid_heights\n",
    "    \n",
    "    print(f\"üìä Edifici totali: {total_buildings}\")\n",
    "    print(f\"üìä Altezze valide (>{config.MIN_VALID_HEIGHT}m): {valid_heights}\")\n",
    "    print(f\"üìä Altezze non valide: {invalid_heights}\")\n",
    "    \n",
    "    if invalid_heights > 0:\n",
    "        error_handler.add_warning(\n",
    "            \"INVALID_HEIGHTS\", \n",
    "            f\"{invalid_heights} edifici con altezza ‚â§ {config.MIN_VALID_HEIGHT}m saranno saltati\"\n",
    "        )\n",
    "    \n",
    "    # =================================================================\n",
    "    # FASE 2: PROCESSING EDIFICI CON ERROR HANDLING\n",
    "    # =================================================================\n",
    "    print(f\"\\n=== FASE 2: PROCESSING EDIFICI ===\")\n",
    "    \n",
    "    results = []\n",
    "    stats = {\n",
    "        'processed_count': 0,\n",
    "        'skipped_invalid_height': 0,\n",
    "        'skipped_no_overlap': 0,\n",
    "        'skipped_geometry_error': 0,\n",
    "        'skipped_other_error': 0\n",
    "    }\n",
    "    \n",
    "    # Loop principale con error handling robusto\n",
    "    for idx, row in vector.iterrows():\n",
    "        building_id = f\"building_{idx}\"\n",
    "        \n",
    "        try:\n",
    "            # Pre-validazione altezza\n",
    "            geom = row.geometry\n",
    "            h_uvl = row[config.HEIGHT_FIELD]\n",
    "            \n",
    "            if h_uvl <= config.MIN_VALID_HEIGHT:\n",
    "                stats['skipped_invalid_height'] += 1\n",
    "                # Crea record con valori zero per altezze non valide\n",
    "                result_record = _create_empty_result_record(\n",
    "                    idx=idx, \n",
    "                    row=row, \n",
    "                    config=config,\n",
    "                    fid_field=fid_field,\n",
    "                    fid_value_source=fid_value_source,\n",
    "                    reason=\"invalid_height\"\n",
    "                )\n",
    "                results.append(result_record)\n",
    "                continue\n",
    "            \n",
    "            # Calcoli geometrici base\n",
    "            a_base = geom.area\n",
    "            vol = a_base * h_uvl\n",
    "            \n",
    "            # Estrazione valori con error handling\n",
    "            try:\n",
    "                external_values = get_external_pixels(geom, raster, config.BUFFER_DISTANCE)\n",
    "                \n",
    "                if external_values.size > 0:\n",
    "                    # Calcola statistiche sommersione\n",
    "                    depth_mean = float(np.mean(external_values))\n",
    "                    depth_min = float(np.min(external_values))\n",
    "                    depth_max = float(np.max(external_values))\n",
    "                    \n",
    "                    # Calcola percentuale sommersione con cap\n",
    "                    perc_submerged = min((depth_mean / h_uvl) * 100, config.MAX_SUBMERSION_PERCENT)\n",
    "                    \n",
    "                    stats['processed_count'] += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Nessuna sovrapposizione\n",
    "                    stats['skipped_no_overlap'] += 1\n",
    "                    depth_mean = depth_min = depth_max = perc_submerged = 0.0\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Errore nell'estrazione pixel\n",
    "                error_handler.handle_processing_error(building_id, \"pixel_extraction\", e)\n",
    "                stats['skipped_other_error'] += 1\n",
    "                depth_mean = depth_min = depth_max = perc_submerged = 0.0\n",
    "            \n",
    "            # Crea record risultato\n",
    "            result_record = _create_result_record(\n",
    "                idx=idx,\n",
    "                row=row,\n",
    "                config=config,\n",
    "                fid_field=fid_field,\n",
    "                fid_value_source=fid_value_source,\n",
    "                a_base=a_base,\n",
    "                h_uvl=h_uvl,\n",
    "                vol=vol,\n",
    "                depth_mean=depth_mean,\n",
    "                depth_min=depth_min,\n",
    "                depth_max=depth_max,\n",
    "                perc_submerged=perc_submerged,\n",
    "                geom=geom\n",
    "            )\n",
    "            \n",
    "            results.append(result_record)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Errore generale processing edificio\n",
    "            error_handler.handle_processing_error(building_id, \"general_processing\", e)\n",
    "            stats['skipped_other_error'] += 1\n",
    "            \n",
    "            # Crea record vuoto per mantenere consistenza\n",
    "            try:\n",
    "                empty_record = _create_empty_result_record(\n",
    "                    idx=idx,\n",
    "                    row=row, \n",
    "                    config=config,\n",
    "                    fid_field=fid_field,\n",
    "                    fid_value_source=fid_value_source,\n",
    "                    reason=\"processing_error\"\n",
    "                )\n",
    "                results.append(empty_record)\n",
    "            except:\n",
    "                pass  # Fallback silenzioso\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % config.PROGRESS_INTERVAL == 0:\n",
    "            print(f\"üìä Elaborati {idx + 1}/{total_buildings} edifici...\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # FASE 3: SUMMARY E VALIDAZIONE FINALE\n",
    "    # =================================================================\n",
    "    print(f\"\\n=== FASE 3: SUMMARY RISULTATI ===\")\n",
    "    print(f\"‚úÖ Edifici processati con successo: {stats['processed_count']}\")\n",
    "    print(f\"‚ö†Ô∏è  Edifici saltati per altezza non valida: {stats['skipped_invalid_height']}\")\n",
    "    print(f\"‚ö†Ô∏è  Edifici saltati per mancanza sovrapposizione: {stats['skipped_no_overlap']}\")\n",
    "    print(f\"‚ùå Edifici saltati per errori geometrici: {stats['skipped_geometry_error']}\")\n",
    "    print(f\"‚ùå Edifici saltati per altri errori: {stats['skipped_other_error']}\")\n",
    "    print(f\"üìä Record risultato totali: {len(results)}\")\n",
    "    \n",
    "    return results, stats\n",
    "\n",
    "def _create_result_record(idx, row, config, fid_field, fid_value_source, \n",
    "                         a_base, h_uvl, vol, depth_mean, depth_min, depth_max, \n",
    "                         perc_submerged, geom):\n",
    "    \"\"\"Crea record risultato standard\"\"\"\n",
    "    \n",
    "    # Gestione FID\n",
    "    if fid_value_source == 'input':\n",
    "        fid_value = row[fid_field]\n",
    "    else:\n",
    "        fid_value = idx + 1\n",
    "    \n",
    "    return {\n",
    "        'FID': fid_value,\n",
    "        'A_BASE': round(float(a_base), 2),\n",
    "        config.HEIGHT_FIELD: round(float(h_uvl), 2),\n",
    "        'VOL': round(float(vol), 2),\n",
    "        'DEPTH_MEAN': round(float(depth_mean), 2),\n",
    "        'DEPTH_MIN': round(float(depth_min), 2),\n",
    "        'DEPTH_MAX': round(float(depth_max), 2),\n",
    "        'PERC_SUBM': round(float(perc_submerged), 2),\n",
    "        'geometry': geom\n",
    "    }\n",
    "\n",
    "def _create_empty_result_record(idx, row, config, fid_field, fid_value_source, reason=\"unknown\"):\n",
    "    \"\"\"Crea record vuoto per edifici non processabili\"\"\"\n",
    "    \n",
    "    try:\n",
    "        geom = row.geometry\n",
    "        a_base = geom.area\n",
    "        h_uvl = row[config.HEIGHT_FIELD] if config.HEIGHT_FIELD in row else 0.0\n",
    "        vol = a_base * h_uvl if h_uvl > 0 else 0.0\n",
    "    except:\n",
    "        # Fallback estremo\n",
    "        geom = row.geometry if hasattr(row, 'geometry') else None\n",
    "        a_base = h_uvl = vol = 0.0\n",
    "    \n",
    "    # Gestione FID\n",
    "    if fid_value_source == 'input' and fid_field and fid_field in row:\n",
    "        fid_value = row[fid_field]\n",
    "    else:\n",
    "        fid_value = idx + 1\n",
    "    \n",
    "    return {\n",
    "        'FID': fid_value,\n",
    "        'A_BASE': round(float(a_base), 2),\n",
    "        config.HEIGHT_FIELD: round(float(h_uvl), 2),\n",
    "        'VOL': round(float(vol), 2),\n",
    "        'DEPTH_MEAN': 0.0,\n",
    "        'DEPTH_MIN': 0.0,\n",
    "        'DEPTH_MAX': 0.0,\n",
    "        'PERC_SUBM': 0.0,\n",
    "        'geometry': geom\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Workflow modulare robusto definito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESECUZIONE WORKFLOW MODULARE\n",
    "print(\"üöÄ AVVIO WORKFLOW AVANZATO\")\n",
    "\n",
    "# Verifica che error_handler possa continuare (no errori critici)\n",
    "if not error_handler.should_continue_processing():\n",
    "    print(\"‚ùå ERRORI CRITICI RILEVATI - Interrompo processing\")\n",
    "    error_handler.print_final_report()\n",
    "    raise FloodAnalysisError(\"Processing interrotto per errori critici nella configurazione\")\n",
    "\n",
    "# Esegui workflow modulare con configurazione avanzata\n",
    "results, processing_stats = run_flood_analysis_workflow(\n",
    "    config=flood_config,\n",
    "    vector=vector,\n",
    "    raster=raster, \n",
    "    error_handler=error_handler\n",
    ")\n",
    "\n",
    "# Aggiorna variabili per backward compatibility\n",
    "processed_count = processing_stats['processed_count']\n",
    "not_processed_count = (\n",
    "    processing_stats['skipped_invalid_height'] + \n",
    "    processing_stats['skipped_no_overlap'] + \n",
    "    processing_stats['skipped_geometry_error'] + \n",
    "    processing_stats['skipped_other_error']\n",
    ")\n",
    "no_overlap_count = processing_stats['skipped_no_overlap']\n",
    "zero_height_count = processing_stats['skipped_invalid_height']\n",
    "other_errors_count = processing_stats['skipped_other_error']\n",
    "\n",
    "print(f\"\\nüéØ WORKFLOW AVANZATO COMPLETATO\")\n",
    "print(f\"üìä Statistiche processing aggiornate:\")\n",
    "print(f\"  - Successi: {processed_count}\")\n",
    "print(f\"  - Fallimenti: {not_processed_count}\")\n",
    "print(f\"  - Record totali: {len(results)}\")\n",
    "\n",
    "# Report errori finale\n",
    "error_handler.print_final_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884723da",
   "metadata": {},
   "source": [
    "## 6. Preparazione Output\n",
    "\n",
    "Creazione del GeoDataFrame di output con i risultati dell'analisi di sommersione elaborati dal workflow avanzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea GeoDataFrame di output\n",
    "out_gdf = gpd.GeoDataFrame(results, crs=vector.crs)\n",
    "total_buildings = len(vector)  # Variabile necessaria per il report\n",
    "\n",
    "print(f\"‚úì GeoDataFrame di output pronto: {len(out_gdf)} record con campi analisi aggiunti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040db3ad",
   "metadata": {},
   "source": [
    "## 7. Generazione Output\n",
    "\n",
    "Creazione del DataFrame di output con schema appropriato e scrittura nel dataset Dataiku di destinazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6358d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti GeoDataFrame in DataFrame standard per Dataiku\n",
    "output_inondazioni_df = pd.DataFrame(out_gdf.drop(columns='geometry'))\n",
    "\n",
    "# Aggiungi colonna WKT come prima colonna per il CSV\n",
    "output_inondazioni_df.insert(0, 'geometry_wkt', out_gdf['geometry'].apply(lambda x: x.wkt))\n",
    "\n",
    "# Assicura che FID sia la seconda colonna\n",
    "if 'FID' in output_inondazioni_df.columns:\n",
    "    fid_col = output_inondazioni_df.pop('FID')\n",
    "    output_inondazioni_df.insert(1, 'FID', fid_col)\n",
    "\n",
    "print(f\"üìã DataFrame per output Dataiku:\")\n",
    "print(f\"   Righe: {len(output_inondazioni_df)}\")\n",
    "print(f\"   Colonne: {len(output_inondazioni_df.columns)}\")\n",
    "print(f\"   Colonne: {list(output_inondazioni_df.columns)}\")\n",
    "\n",
    "# Mostra esempio primi record\n",
    "print(f\"\\nüìä Esempio primi 3 record:\")\n",
    "print(output_inondazioni_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio risultati come CSV (condizionale)\n",
    "if flood_config.CREATE_REPORT:\n",
    "    print(\"üìù Salvataggio file CSV...\")\n",
    "\n",
    "    italian_tz = pytz.timezone('Europe/Rome')\n",
    "    timestamp = datetime.now(italian_tz).strftime(\"%Y%m%d_%H%M%S\") \n",
    "    csv_filename = f\"risultati_inondazioni_{timestamp}.csv\"\n",
    "\n",
    "    try:\n",
    "        output_folder = dataiku.Folder(\"output_inondazioni\")\n",
    "        csv_content = output_inondazioni_df.to_csv(index=False)\n",
    "        \n",
    "        from io import StringIO\n",
    "        csv_stream = StringIO(csv_content)\n",
    "        output_folder.upload_stream(csv_filename, csv_stream.getvalue().encode('utf-8'))\n",
    "        \n",
    "        print(f\"‚úÖ File CSV salvato: {csv_filename}\")\n",
    "        print(f\"‚úÖ {len(output_inondazioni_df)} record salvati\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        local_csv = f\"C:\\\\temp\\\\{csv_filename}\"\n",
    "        output_inondazioni_df.to_csv(local_csv, index=False)\n",
    "        print(f\"‚úÖ File salvato in locale: {local_csv}\")\n",
    "        print(f\"‚úÖ {len(output_inondazioni_df)} record salvati\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Salvataggio CSV disabilitato (create_report=false)\")\n",
    "\n",
    "print(\"‚úÖ ELABORAZIONE COMPLETATA\")\n",
    "print(f\"‚úÖ Dataset 'output_inondazioni' scritto con {len(output_inondazioni_df)} record\")\n",
    "print(f\"‚úÖ Analisi di sommersione completata per {processed_count}/{total_buildings} edifici\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51707651",
   "metadata": {},
   "source": [
    "## 8. Salvataggio File Fisici\n",
    "\n",
    "Salvataggio di shapefile, report statistico e upload nel folder Dataiku di output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione folder di output per il salvataggio dei risultati\n",
    "output_folder = dataiku.Folder(\"output_inondazioni\")\n",
    "\n",
    "# Creazione directory temporanea per i file di output\n",
    "output_temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Directory temporanea per output: {output_temp_dir}\")\n",
    "\n",
    "# Generazione nome base per i file di output con timestamp\n",
    "italian_tz = pytz.timezone('Europe/Rome')\n",
    "timestamp = datetime.now(italian_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_base_name = f\"wd_analysis_{timestamp}\"\n",
    "\n",
    "# Definizione percorsi dei file di output\n",
    "shapefile_path = os.path.join(output_temp_dir, f\"{output_base_name}.shp\")\n",
    "report_path = os.path.join(output_temp_dir, f\"{output_base_name}_report.txt\")\n",
    "log_path = os.path.join(output_temp_dir, f\"{output_base_name}.log\")\n",
    "\n",
    "print(f\"File di output programmati:\")\n",
    "print(f\"   Shapefile: {os.path.basename(shapefile_path)}\")\n",
    "print(f\"   Report statistico: {os.path.basename(report_path)}\")\n",
    "print(f\"   Log elaborazione: {os.path.basename(log_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d834d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva vettoriale con schema definito (condizionale)\n",
    "if flood_config.CREATE_SHAPEFILE:\n",
    "    schema = {\n",
    "        'geometry': 'Polygon',\n",
    "        'properties': {\n",
    "            'FID': 'int:10',             # Campo identificativo\n",
    "            'A_BASE': 'float:10.2',      # 10 cifre totali, 2 decimali\n",
    "            HEIGHT_FIELD: 'float:8.2',   # 8 cifre totali, 2 decimali  \n",
    "            'VOL': 'float:12.2',         # 12 cifre totali, 2 decimali\n",
    "            'DEPTH_MEAN': 'float:8.2',   # 8 cifre totali, 2 decimali\n",
    "            'DEPTH_MIN': 'float:8.2',    # 8 cifre totali, 2 decimali\n",
    "            'DEPTH_MAX': 'float:8.2',    # 8 cifre totali, 2 decimali\n",
    "            'PERC_SUBM': 'float:6.2'     # 6 cifre totali, 2 decimali\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"üíæ Salvataggio vettoriale...\")\n",
    "    with fiona.open(shapefile_path, 'w', driver='ESRI Shapefile', crs=out_gdf.crs, schema=schema) as f:\n",
    "        for idx, row in out_gdf.iterrows():\n",
    "            feature = {\n",
    "                'geometry': mapping(row.geometry),\n",
    "                'properties': {\n",
    "                    'FID': int(row['FID']),\n",
    "                    'A_BASE': float(row['A_BASE']),\n",
    "                    HEIGHT_FIELD: float(row[HEIGHT_FIELD]),\n",
    "                    'VOL': float(row['VOL']),\n",
    "                    'DEPTH_MEAN': float(row['DEPTH_MEAN']),\n",
    "                    'DEPTH_MIN': float(row['DEPTH_MIN']),\n",
    "                    'DEPTH_MAX': float(row['DEPTH_MAX']),\n",
    "                    'PERC_SUBM': float(row['PERC_SUBM'])\n",
    "                }\n",
    "            }\n",
    "            f.write(feature)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Salvataggio shapefile disabilitato (create_shapefile=false)\")\n",
    "    shapefile_path = None  # Prevent file operations later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea report statistico dettagliato (condizionale)\n",
    "if flood_config.CREATE_REPORT:\n",
    "    print(\"üìä Generazione report statistico...\")\n",
    "\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== REPORT ANALISI SOMMERSIONE EDIFICI ===\\n\\n\")\n",
    "        f.write(f\"Data elaborazione: {datetime.now(pytz.timezone('Europe/Rome')).strftime('%Y-%m-%d %H:%M:%S')} (Ora italiana)\\n\")\n",
    "        f.write(f\"Versione script: dataiku_integration.ipynb\\n\")\n",
    "        f.write(f\"Campo altezza utilizzato: {HEIGHT_FIELD}\\n\")\n",
    "        f.write(f\"Opzione riproiezione: {REPROJECTION_OPTION} \")\n",
    "        if REPROJECTION_OPTION == 1:\n",
    "            f.write(\"(riproietta vettoriale)\\n\")\n",
    "        elif REPROJECTION_OPTION == 2:\n",
    "            f.write(\"(riproietta raster)\\n\")\n",
    "        else:\n",
    "            f.write(f\"(riproietta entrambi in {TARGET_EPSG})\\n\")\n",
    "        f.write(f\"Buffer distance: {BUFFER_DISTANCE}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=== FILE DI INPUT/OUTPUT ===\\n\")\n",
    "        f.write(f\"File vettoriale: {vector_file}\\n\")\n",
    "        f.write(f\"File raster: {raster_file}\\n\")\n",
    "        f.write(f\"File output shapefile: {os.path.basename(shapefile_path)}\\n\")\n",
    "        f.write(f\"File output report: {os.path.basename(report_path)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=== SISTEMI DI RIFERIMENTO ===\\n\")\n",
    "        f.write(f\"CRS vettoriale originale: {vector_crs}\\n\")\n",
    "        f.write(f\"CRS raster: {raster_crs}\\n\")\n",
    "        if vector_crs != raster_crs:\n",
    "            f.write(\"NOTA: Sistemi di riferimento diversi - applicata riproiezione automatica\\n\")\n",
    "        else:\n",
    "            f.write(\"NOTA: Sistemi di riferimento coincidenti - nessuna riproiezione necessaria\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"=== RIEPILOGO ELABORAZIONE ===\\n\")\n",
    "        f.write(f\"Edifici totali nel vettoriale: {total_buildings}\\n\")\n",
    "        f.write(f\"Edifici processati con successo: {processed_count} ({processed_count/total_buildings*100:.1f}%)\\n\")\n",
    "        f.write(f\"Edifici non processati: {not_processed_count} ({not_processed_count/total_buildings*100:.1f}%)\\n\")\n",
    "        f.write(f\"  - Cause: senza sovrapposizione con raster, altezza zero/negativa, errori geometrici\\n\\n\")\n",
    "        \n",
    "        f.write(\"=== METODOLOGIA ===\\n\")\n",
    "        f.write(\"L'analisi calcola la sommersione degli edifici campionando i valori di profondit√†\\n\")\n",
    "        f.write(\"dell'acqua nei pixel esterni al perimetro di ciascun edificio.\\n\")\n",
    "        f.write(\"La percentuale di sommersione √® calcolata come: (profondit√†_media / altezza_edificio) √ó 100\\n\")\n",
    "        f.write(\"I valori sono limitati al 100% per edifici completamente sommersi.\\n\\n\")\n",
    "        \n",
    "        # Statistiche dettagliate se ci sono edifici processati\n",
    "        if processed_count > 0:\n",
    "            processed_data = out_gdf[out_gdf['DEPTH_MEAN'] > 0]\n",
    "            \n",
    "            if len(processed_data) > 0:\n",
    "                f.write(f\"Edifici con sommersione rilevata: {len(processed_data)} ({len(processed_data)/total_buildings*100:.1f}%)\\n\\n\")\n",
    "                \n",
    "                # Statistiche profondit√†\n",
    "                mean_depth = processed_data['DEPTH_MEAN'].mean()  \n",
    "                max_depth = processed_data['DEPTH_MAX'].max()\n",
    "                min_depth = processed_data['DEPTH_MIN'].min()\n",
    "                \n",
    "                f.write(\"=== PROFONDIT√Ä ACQUA ===\\n\")\n",
    "                f.write(f\"Profondit√† media: {mean_depth:.2f} m (range: {min_depth:.2f} - {max_depth:.2f} m)\\n\\n\")\n",
    "                \n",
    "                # Classificazione edifici\n",
    "                edifici_bassi = len(processed_data[processed_data['PERC_SUBM'] < 25])\n",
    "                edifici_medi = len(processed_data[(processed_data['PERC_SUBM'] >= 25) & (processed_data['PERC_SUBM'] < 75)])\n",
    "                edifici_alti = len(processed_data[processed_data['PERC_SUBM'] >= 75])\n",
    "                \n",
    "                f.write(\"=== CLASSIFICAZIONE EDIFICI PER LIVELLO SOMMERSIONE ===\\n\")\n",
    "                f.write(f\"Sommersione bassa (<25%): {edifici_bassi} edifici ({edifici_bassi/len(processed_data)*100:.1f}%)\\n\")\n",
    "                f.write(f\"Sommersione media (25-75%): {edifici_medi} edifici ({edifici_medi/len(processed_data)*100:.1f}%)\\n\")\n",
    "                f.write(f\"Sommersione alta (‚â•75%): {edifici_alti} edifici ({edifici_alti/len(processed_data)*100:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"=== CAMPI OUTPUT VETTORIALE ===\\n\")\n",
    "        fid_source_text = \"ereditato dall'input\" if fid_value_source == 'input' else 'generato automaticamente'\n",
    "        f.write(f\"FID: Identificativo univoco edificio ({fid_source_text})\\n\")\n",
    "        f.write(\"DEPTH_MEAN: Profondit√† media dell'acqua attorno all'edificio (m)\\n\")\n",
    "        f.write(\"DEPTH_MAX: Profondit√† massima dell'acqua attorno all'edificio (m)\\n\")\n",
    "        f.write(\"DEPTH_MIN: Profondit√† minima dell'acqua attorno all'edificio (m)\\n\")\n",
    "        f.write(\"PERC_SUBM: Percentuale di sommersione dell'edificio (%)\\n\")\n",
    "        f.write(f\"{HEIGHT_FIELD}: Altezza dell'edificio utilizzata nel calcolo (m)\\n\")\n",
    "        f.write(\"A_BASE: Area della base dell'edificio (m¬≤)\\n\")\n",
    "        f.write(\"VOL: Volume dell'edificio (m¬≥)\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Report salvato: {os.path.basename(report_path)}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Generazione report statistico disabilitata (create_report=false)\")\n",
    "    report_path = None  # Prevent file operations later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il file di log con TUTTE le stampe catturate (condizionale)\n",
    "if flood_config.ENABLE_LOGGING:\n",
    "    print(\"üìù Generazione file di log completo...\")\n",
    "    with open(log_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"=== LOG ELABORAZIONE ANALISI SOMMERSIONE EDIFICI ===\\n\\n\")\n",
    "        f.write(f\"Data elaborazione: {datetime.now(pytz.timezone('Europe/Rome')).strftime('%Y-%m-%d %H:%M:%S')} (Ora italiana)\\n\")\n",
    "        f.write(f\"Versione script: dataiku_integration.ipynb\\n\")\n",
    "        f.write(f\"Parametri utilizzati:\\n\")\n",
    "        f.write(f\"  - HEIGHT_FIELD: {HEIGHT_FIELD}\\n\")\n",
    "        f.write(f\"  - REPROJECTION_OPTION: {REPROJECTION_OPTION}\\n\")\n",
    "        f.write(f\"  - TARGET_EPSG: {TARGET_EPSG}\\n\")\n",
    "        f.write(f\"  - BUFFER_DISTANCE: {BUFFER_DISTANCE}\\n\")\n",
    "        f.write(f\"\\n=== TRANSCRIPT ESECUZIONE ===\\n\\n\")\n",
    "        f.write(log_capture.get_log_content())\n",
    "        f.write(f\"\\n=== FINE LOG ===\\n\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Generazione log disabilitata (enable_logging=false)\")\n",
    "    log_path = None  # Prevent file operations later\n",
    "\n",
    "# Upload dei file di output nel folder Dataiku\n",
    "print(\"üì§ Upload file nel folder Dataiku di output...\")\n",
    "\n",
    "# Lista di tutti i file da caricare (shapefile + accessori + report + log)\n",
    "files_to_upload = []\n",
    "\n",
    "# Shapefile principale (se creato)\n",
    "if shapefile_path and os.path.exists(shapefile_path):\n",
    "    files_to_upload.append(shapefile_path)\n",
    "    # File accessori shapefile\n",
    "    for ext in ['.dbf', '.shx', '.prj', '.cpg']:\n",
    "        aux_file = shapefile_path.replace('.shp', ext)\n",
    "        if os.path.exists(aux_file):\n",
    "            files_to_upload.append(aux_file)\n",
    "\n",
    "# Report (se creato)\n",
    "if report_path and os.path.exists(report_path):\n",
    "    files_to_upload.append(report_path)\n",
    "\n",
    "# File di log (se abilitato)\n",
    "if log_path and os.path.exists(log_path):\n",
    "    files_to_upload.append(log_path)\n",
    "\n",
    "# Upload dei file\n",
    "uploaded_files = []\n",
    "for file_path in files_to_upload:\n",
    "    if os.path.exists(file_path):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                output_folder.upload_stream(file_name, f)\n",
    "            uploaded_files.append(file_name)\n",
    "        except Exception as e:\n",
    "            pass  # Gestione silenziosa degli errori\n",
    "\n",
    "# Aggiorna il log con le informazioni di upload\n",
    "with open(log_path, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"\\n=== OPERAZIONI DI UPLOAD ===\\n\\n\")\n",
    "    f.write(f\"üì§ Upload completato nel folder 'minio/output'\\n\")\n",
    "    f.write(f\"üìÅ File caricati: {len(uploaded_files)}\\n\")\n",
    "    for filename in uploaded_files:\n",
    "        f.write(f\"   ‚úÖ {filename}\\n\")\n",
    "    f.write(f\"\\nüéâ SALVATAGGIO COMPLETATO!\\n\")\n",
    "\n",
    "# Re-upload del log aggiornato\n",
    "try:\n",
    "    with open(log_path, 'rb') as f:\n",
    "        output_folder.upload_stream(os.path.basename(log_path), f)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Pulizia directory temporanea output  \n",
    "try:\n",
    "    shutil.rmtree(output_temp_dir)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6894601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulizia risorse e file temporanei\n",
    "try:\n",
    "    raster.close()\n",
    "    if 'temp_raster_path' in locals():\n",
    "        try:\n",
    "            os.unlink(temp_raster_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Pulizia directory temporanea\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Ripristina stdout originale e chiudi il sistema di logging\n",
    "sys.stdout = log_capture.original_stdout\n",
    "log_capture.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINALIZZAZIONE AVANZATA E EXIT CODE\n",
    "# Determina exit code basato sui risultati con logica avanzata\n",
    "try:\n",
    "    # Ottieni summary errori dal error handler\n",
    "    error_summary = error_handler.get_error_summary()\n",
    "    \n",
    "    if 'processed_count' in locals() and processed_count > 0:\n",
    "        # Successo con eventuali warning\n",
    "        if error_summary['has_critical_errors']:\n",
    "            exit_code = 2  # Successo con errori critici\n",
    "            print(f\"‚ö†Ô∏è  ANALISI COMPLETATA CON ERRORI CRITICI - Exit Code: {exit_code}\")\n",
    "        elif error_summary['total_errors'] > 0:\n",
    "            exit_code = 1  # Successo con errori non critici\n",
    "            print(f\"‚ö†Ô∏è  ANALISI COMPLETATA CON ERRORI - Exit Code: {exit_code}\")\n",
    "        else:\n",
    "            exit_code = 0  # Successo completo\n",
    "            print(f\"‚úÖ ANALISI SOMMERSIONE COMPLETATA - Exit Code: {exit_code}\")\n",
    "        \n",
    "        print(f\"üìä Risultati: {processed_count}/{len(vector) if 'vector' in locals() else 0} edifici elaborati con successo\")\n",
    "        print(f\"üìÅ Output generati: CSV, Shapefile, Report e Log\")\n",
    "        print(f\"üîß Errori tracciati: {error_summary['total_errors']}\")\n",
    "        print(f\"‚ö†Ô∏è  Warning: {error_summary['total_warnings']}\")\n",
    "        \n",
    "    else:\n",
    "        exit_code = 3  # Fallimento completo\n",
    "        print(f\"‚ùå ANALISI FALLITA - Exit Code: {exit_code}\")\n",
    "        print(f\"üî• Nessun edificio elaborato con successo\")\n",
    "        print(f\"üîß Errori critici: {error_summary.get('stats', {}).get('validation_errors', 0)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    exit_code = 4  # Errore critico sistema\n",
    "    print(f\"‚ùå ERRORE CRITICO SISTEMA - Exit Code: {exit_code}\")\n",
    "    print(f\"üî• Errore: {str(e)}\")\n",
    "\n",
    "# Compatibilit√† con architettura payload avanzato\n",
    "print(f\"\\nüéâ PROCEDURA AVANZATA TERMINATA - Exit Code: {exit_code}\")\n",
    "print(f\"üìã Payload ID elaborazione: {flood_payload.get('elab_id') if 'flood_payload' in locals() else 'N/A'}\")\n",
    "print(f\"üîß Config ID: {flood_config.ELAB_ID if 'flood_config' in locals() else 'N/A'}\")\n",
    "\n",
    "# Sistema exit code avanzato:\n",
    "# 0 = Successo completo\n",
    "# 1 = Successo con errori non critici  \n",
    "# 2 = Successo con errori critici\n",
    "# 3 = Fallimento completo\n",
    "# 4 = Errore critico sistema\n",
    "\n",
    "# Per compatibilit√† futura con script autonomo\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.exit(exit_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ee1e4",
   "metadata": {},
   "source": [
    "## üß™ Test Scenario JSON - Area di Testing\n",
    "\n",
    "**IMPORTANTE**: Questa sezione √® solo per testing. Non influenza l'esecuzione normale del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b160e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ TESTER SCENARIO JSON\n",
    "# Questa cella simula l'esecuzione di uno scenario senza modificare il workflow principale\n",
    "\n",
    "def test_scenario_parsing(test_json_str):\n",
    "    \"\"\"\n",
    "    Testa il parsing di un JSON scenario senza eseguire l'analisi completa\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    print(\"üß™ === TEST SCENARIO JSON ===\")\n",
    "    print(f\"JSON di input:\\n{test_json_str}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON\n",
    "        scenario_params = json.loads(test_json_str)\n",
    "        print(\"‚úÖ JSON parsing riuscito\")\n",
    "        \n",
    "        # Simula creazione payload scenario\n",
    "        payload = scenario_params.copy()\n",
    "        if payload.get('elab_id') is None:\n",
    "            payload[\"elab_id\"] = f\"flood_test_{datetime.now(pytz.timezone('Europe/Rome')).strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"üìã Payload generato: {list(payload.keys())}\")\n",
    "        \n",
    "        # Testa FloodAnalysisConfig con il payload\n",
    "        test_config = FloodAnalysisConfig(payload)\n",
    "        \n",
    "        # ‚ö†Ô∏è CONTROLLO WARNING - Sistema di diagnostica JSON\n",
    "        warnings = test_config.get_configuration_warnings()\n",
    "        if warnings:\n",
    "            print(f\"\\n‚ö†Ô∏è === WARNING DI CONFIGURAZIONE ===\")\n",
    "            for i, warning in enumerate(warnings, 1):\n",
    "                print(f\"‚ö†Ô∏è WARNING {i}: {warning['type']}\")\n",
    "                print(f\"   üìã {warning['message']}\")\n",
    "                print(f\"   ‚ÑπÔ∏è  {warning['details']}\")\n",
    "                print(f\"   üí° {warning['suggestion']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ === NESSUN WARNING - Configurazione ottimale ===\")\n",
    "        \n",
    "        print(\"\\nüîß === CONFIGURAZIONE RISULTANTE ===\")\n",
    "        test_config.print_config()\n",
    "        \n",
    "        # Validazione\n",
    "        errors = test_config.validate_config()\n",
    "        if errors:\n",
    "            print(f\"\\n‚ùå ERRORI DI VALIDAZIONE:\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ VALIDAZIONE SUPERATA - Configurazione valida\")\n",
    "            \n",
    "        print(f\"\\nüìÅ File configurati:\")\n",
    "        print(f\"  - Vettoriale: {test_config.INPUT_VECTOR_FILE or 'N/A'}\")\n",
    "        print(f\"  - Raster: {test_config.INPUT_RASTER_FILE or 'N/A'}\")\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è Parametri di controllo:\")\n",
    "        print(f\"  - Logging: {test_config.ENABLE_LOGGING}\")\n",
    "        print(f\"  - Report: {test_config.CREATE_REPORT}\")\n",
    "        print(f\"  - Shapefile: {test_config.CREATE_SHAPEFILE}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå ERRORE JSON: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRORE CONFIGURAZIONE: {e}\")\n",
    "        return False\n",
    "\n",
    "# ===== ESEMPI DI TEST =====\n",
    "\n",
    "# Test 1: Scenario BASE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "test_scenario_1 = '''\n",
    "{\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici.shp\",\n",
    "    \"raster\": \"flood_depth.tif\"\n",
    "  }\n",
    "}\n",
    "'''\n",
    "test_scenario_parsing(test_scenario_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Scenario COMPLETO\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "test_scenario_2 = '''\n",
    "{\n",
    "  \"elab_id\": \"flood_analysis_complete_001\",\n",
    "  \"event_name\": \"Alluvione Tevere Ottobre 2024\",\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici_roma.shp\",\n",
    "    \"raster\": \"flood_depth_tevere.tif\"\n",
    "  },\n",
    "  \"HEIGHT_FIELD\": \"H_UVL\",\n",
    "  \"TARGET_EPSG\": \"32632\",\n",
    "  \"REPROJECTION_OPTION\": 1,\n",
    "  \"BUFFER_DISTANCE\": \"auto\",\n",
    "  \"min_valid_height\": 0.5,\n",
    "  \"enable_logging\": true,\n",
    "  \"create_report\": true,\n",
    "  \"create_shapefile\": false,\n",
    "  \"output_naming\": {\n",
    "    \"dataset_name\": \"risultati_alluvione_tevere\",\n",
    "    \"folder_name\": \"output_tevere_analisi\",\n",
    "    \"file_prefix\": \"tevere_\",\n",
    "    \"file_suffix\": \"final\",\n",
    "    \"include_timestamp\": true\n",
    "  }\n",
    "}\n",
    "'''\n",
    "test_scenario_parsing(test_scenario_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Scenario con ERRORI (per testare validazione)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "test_scenario_3 = '''\n",
    "{\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici.shp\",\n",
    "    \"raster\": \"flood.tif\"\n",
    "  },\n",
    "  \"HEIGHT_FIELD\": \"\",\n",
    "  \"REPROJECTION_OPTION\": 5,\n",
    "  \"BUFFER_DISTANCE\": -1.5,\n",
    "  \"create_report\": \"maybe\",\n",
    "  \"min_valid_height\": \"invalid\"\n",
    "}\n",
    "'''\n",
    "test_scenario_parsing(test_scenario_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200eb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® TEST NAMING PERSONALIZZATO - Verifica sistema di naming custom\n",
    "import json  # Import necessario per il test\n",
    "\n",
    "print(\"üé®\" + \"=\"*60)\n",
    "print(\"üé® === TEST NAMING PERSONALIZZATO ===\")\n",
    "\n",
    "# Scenario con naming personalizzato completo\n",
    "test_scenario_naming = {\n",
    "    \"files\": {\n",
    "        \"vettoriale\": \"edifici_tevere.shp\",\n",
    "        \"raster\": \"flood_depth_tevere.tif\"\n",
    "    },\n",
    "    \"event_name\": \"Alluvione_Tevere_2024\",\n",
    "    \"output_naming\": {\n",
    "        \"dataset_name\": \"risultati_alluvione_tevere\",\n",
    "        \"folder_name\": \"output_tevere_analisi\", \n",
    "        \"file_prefix\": \"tevere_flood_\",\n",
    "        \"file_suffix\": \"final\",\n",
    "        \"include_timestamp\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"JSON di input:\")\n",
    "print()\n",
    "print(json.dumps(test_scenario_naming, indent=2))\n",
    "print()\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Test configurazione con naming personalizzato\n",
    "    config_naming = FloodAnalysisConfig(test_scenario_naming)\n",
    "    \n",
    "    print(\"‚úÖ JSON parsing riuscito\")\n",
    "    print(f\"üìã Payload generato: {list(test_scenario_naming.keys())}\")\n",
    "    print()\n",
    "    print(\"üîß === CONFIGURAZIONE CON NAMING PERSONALIZZATO ===\")\n",
    "    print()\n",
    "    \n",
    "    # Mostra configurazione\n",
    "    config_naming.print_config()\n",
    "    \n",
    "    # Test validazione\n",
    "    naming_errors = config_naming.validate_config()\n",
    "    if naming_errors:\n",
    "        print(\"‚ùå ERRORI DI VALIDAZIONE:\")\n",
    "        for err in naming_errors:\n",
    "            print(f\"  - {err}\")\n",
    "    else:\n",
    "        print(\"‚úÖ VALIDAZIONE SUPERATA - Configurazione valida\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üé® Naming personalizzato configurato!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRORE: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ TEST - Parsing Case-Insensitive\n",
    "import json\n",
    "\n",
    "print(\"üéØ\" + \"=\"*60)\n",
    "print(\"üéØ === DEMO CASE-INSENSITIVE PARSING ===\")\n",
    "\n",
    "# Test rapido con diverse variazioni di case\n",
    "demo_payload = {\n",
    "    \"HEIGHT_FIELD\": \"h_uvl\",           # MAIUSCOLO\n",
    "    \"target_epsg\": \"32632\",            # minuscolo  \n",
    "    \"Buffer_Distance\": \"auto\",         # MixedCase\n",
    "    \"REPROJECTION_option\": 1,          # Mix con underscore\n",
    "    \"Event_Name\": \"Demo Test\",         # CamelCase\n",
    "    \"create_REPORT\": True,             # Mix maiusc/minusc\n",
    "    \"Files\": {                         # Sezione Files maiuscola\n",
    "        \"VETTORIALE\": \"demo.shp\",      # Chiave maiuscola\n",
    "        \"raster\": \"demo.tif\"           # Chiave minuscola\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"JSON di input con case misto:\")\n",
    "print(json.dumps(demo_payload, indent=2))\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Test configurazione\n",
    "    demo_config = FloodAnalysisConfig(demo_payload)\n",
    "    \n",
    "    # Mostra risultati\n",
    "    warnings = demo_config.get_configuration_warnings()\n",
    "    if warnings:\n",
    "        print(\"‚ö†Ô∏è Warning trovati:\")\n",
    "        for w in warnings:\n",
    "            print(f\"  - {w['message']}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Nessun warning - Parsing perfetto!\")\n",
    "    \n",
    "    print(f\"\\nüîß Parametri caricati correttamente:\")\n",
    "    print(f\"  - HEIGHT_FIELD: {demo_config.HEIGHT_FIELD}\")\n",
    "    print(f\"  - TARGET_EPSG: {demo_config.TARGET_EPSG}\")\n",
    "    print(f\"  - BUFFER_DISTANCE: {demo_config.BUFFER_DISTANCE}\")\n",
    "    print(f\"  - REPROJECTION_OPTION: {demo_config.REPROJECTION_OPTION}\")\n",
    "    print(f\"  - EVENT_NAME: {demo_config.EVENT_NAME}\")\n",
    "    print(f\"  - CREATE_REPORT: {demo_config.CREATE_REPORT}\")\n",
    "    print(f\"  - INPUT_VECTOR_FILE: {demo_config.INPUT_VECTOR_FILE}\")\n",
    "    print(f\"  - INPUT_RASTER_FILE: {demo_config.INPUT_RASTER_FILE}\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESSO: Tutti i parametri sono stati interpretati correttamente!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a139359",
   "metadata": {},
   "source": [
    "# üìã Riferimento Completo Parametri Scenario\n",
    "\n",
    "## Parametri Principali\n",
    "\n",
    "| **Parametro JSON** | **Tipo** | **Descrizione** | **Valore Default** | **Esempio** |\n",
    "|-------------------|----------|----------------|-------------------|-------------|\n",
    "| `elab_id` | string | Identificativo univoco dell'elaborazione | Auto-generato `flood_YYYYMMDD_HHMMSS` | `\"flood_analysis_001\"` |\n",
    "| `event_name` | string | Nome descrittivo dell'evento alluvionale | `null` | `\"Alluvione_Tevere_2024\"` |\n",
    "\n",
    "## File di Input\n",
    "\n",
    "| **Parametro JSON** | **Tipo** | **Descrizione** | **Valore Default** | **Esempio** |\n",
    "|-------------------|----------|----------------|-------------------|-------------|\n",
    "| `files.vettoriale` | string | Nome file shapefile edifici | Da dataset `configurazione_dati` | `\"edifici_roma.shp\"` |\n",
    "| `files.raster` | string | Nome file raster profondit√† acqua | Da dataset `configurazione_dati` | `\"flood_depth.tif\"` |\n",
    "\n",
    "## Parametri Tecnici Core\n",
    "\n",
    "| **Parametro JSON** | **Tipo** | **Descrizione** | **Valore Default** | **Esempio** |\n",
    "|-------------------|----------|----------------|-------------------|-------------|\n",
    "| `HEIGHT_FIELD` | string | Campo altezza edifici nello shapefile | Da dataset `configurazione_parametri` | `\"H_UVL\"` |\n",
    "| `TARGET_EPSG` | string | Sistema di coordinate di destinazione | Da dataset `configurazione_parametri` | `\"32632\"` |\n",
    "| `REPROJECTION_OPTION` | int | Modalit√† riproiezione (1=vettoriale‚Üíraster, 2=raster‚Üívettoriale, 3=entrambi‚ÜíEPSG) | Da dataset `configurazione_parametri` | `1` |\n",
    "| `BUFFER_DISTANCE` | float/string | Distanza buffer in metri (o \"auto\") | Da dataset `configurazione_parametri` | `2.5` o `\"auto\"` |\n",
    "\n",
    "## Parametri di Processamento\n",
    "\n",
    "| **Parametro JSON** | **Tipo** | **Descrizione** | **Valore Default** | **Esempio** |\n",
    "|-------------------|----------|----------------|-------------------|-------------|\n",
    "| `min_valid_height` | float | Altezza minima valida edifici (metri) | `3.0` | `0.5` |\n",
    "\n",
    "## Controlli di Output\n",
    "\n",
    "| **Parametro JSON** | **Tipo** | **Descrizione** | **Valore Default** | **Esempio** |\n",
    "|-------------------|----------|----------------|-------------------|-------------|\n",
    "| `enable_logging` | boolean | Attiva logging dettagliato | `true` | `false` |\n",
    "| `create_report` | boolean | Genera report statistico HTML | `true` | `false` |\n",
    "| `create_shapefile` | boolean | Salva risultati come shapefile | `true` | `false` |\n",
    "\n",
    "## Naming Personalizzato (Opzionale)\n",
    "\n",
    "| **Parametro JSON** | **Tipo** | **Descrizione** | **Valore Default** | **Esempio** |\n",
    "|-------------------|----------|----------------|-------------------|-------------|\n",
    "| `output_naming.dataset_name` | string | Nome personalizzato dataset output | `null` (usa `\"output_inondazioni\"`) | `\"risultati_tevere_2024\"` |\n",
    "| `output_naming.folder_name` | string | Nome personalizzato folder output | `null` (usa `\"output_inondazioni\"`) | `\"analisi_tevere\"` |\n",
    "| `output_naming.file_prefix` | string | Prefisso per file generati | `null` | `\"tevere_flood_\"` |\n",
    "| `output_naming.file_suffix` | string | Suffisso per file generati | `null` | `\"_final\"` |\n",
    "| `output_naming.include_timestamp` | boolean | Include timestamp nei nomi | `true` | `false` |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Note sui Valori Default\n",
    "\n",
    "### **Gerarchia di Priorit√†**:\n",
    "1. **Parametri JSON Scenario** (priorit√† massima)\n",
    "2. **Dataset Dataiku** (`configurazione_parametri`, `configurazione_dati`)\n",
    "3. **Valori hardcoded** (priorit√† minima)\n",
    "\n",
    "### **Parametri Obbligatori**:\n",
    "- `HEIGHT_FIELD` - deve essere sempre specificato\n",
    "- `TARGET_EPSG` - deve essere sempre specificato  \n",
    "- `REPROJECTION_OPTION` - deve essere 1, 2 o 3\n",
    "- `files.vettoriale` e `files.raster` - devono esistere\n",
    "\n",
    "### **Valori Speciali**:\n",
    "- `BUFFER_DISTANCE: \"auto\"` - usa risoluzione spaziale del raster\n",
    "- `REPROJECTION_OPTION: 1` - riproietta vettoriale nel CRS del raster\n",
    "- `REPROJECTION_OPTION: 2` - riproietta raster nel CRS del vettoriale\n",
    "- `REPROJECTION_OPTION: 3` - riproietta entrambi nel CRS specificato da TARGET_EPSG\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Esempio JSON Completo\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"elab_id\": \"flood_analysis_complete_001\",\n",
    "  \"event_name\": \"Alluvione Tevere Ottobre 2024\",\n",
    "  \"files\": {\n",
    "    \"vettoriale\": \"edifici_roma.shp\",\n",
    "    \"raster\": \"flood_depth_tevere.tif\"\n",
    "  },\n",
    "  \"HEIGHT_FIELD\": \"H_UVL\",\n",
    "  \"TARGET_EPSG\": \"32632\",\n",
    "  \"REPROJECTION_OPTION\": 1,\n",
    "  \"BUFFER_DISTANCE\": \"auto\",\n",
    "  \"min_valid_height\": 0.5,\n",
    "  \"enable_logging\": true,\n",
    "  \"create_report\": true,\n",
    "  \"create_shapefile\": false,\n",
    "  \"output_naming\": {\n",
    "    \"dataset_name\": \"risultati_alluvione_tevere\",\n",
    "    \"folder_name\": \"output_tevere_analisi\",\n",
    "    \"file_prefix\": \"tevere_\",\n",
    "    \"file_suffix\": \"final\",\n",
    "    \"include_timestamp\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## üîÑ Note sul Sistema Case-Insensitive\n",
    "\n",
    "\n",
    "### ‚úÖ Esempi supportati:\n",
    "```json\n",
    "{\n",
    "  \"HEIGHT_FIELD\": \"h_uvl\",       // ‚úÖ Maiuscolo\n",
    "  \"height_field\": \"h_uvl\",       // ‚úÖ Minuscolo  \n",
    "  \"Height_Field\": \"h_uvl\",       // ‚úÖ CamelCase\n",
    "  \"FILES\": {...},                // ‚úÖ Maiuscolo\n",
    "  \"files\": {...},                // ‚úÖ Minuscolo\n",
    "  \"Files\": {...}                 // ‚úÖ Capitalizzato\n",
    "}\n",
    "```\n",
    "\n",
    "## üìÅ Differenza OUTPUT_DATASET_NAME vs OUTPUT_FOLDER_NAME\n",
    "\n",
    "- **Se servono solo i file** (.shp, .csv) ‚Üí usa solo `OUTPUT_FOLDER_NAME`\n",
    "- **Se si lavora in Dataiku** ‚Üí usa `OUTPUT_DATASET_NAME` per organizzare il catalogo\n",
    "\n",
    "### **üìÇ OUTPUT_FOLDER_NAME** (quella che si usa di solito)\n",
    "- Nome della **cartella fisica** con i file esportati\n",
    "- Esempio: `\"export_comune\"` ‚Üí cartella `export_comune_20241014_143022/`\n",
    "\n",
    "### **üìä OUTPUT_DATASET_NAME** (solo per Dataiku)\n",
    "- Nome del dataset nel catalogo interno di Dataiku\n",
    "- Se non specifichi: usa nome generico `output_inondazioni_TIMESTAMP`\n",
    "\n",
    "### üîß Esempi pratici:\n",
    "\n",
    "**Caso tipico** (solo file):\n",
    "```json\n",
    "{\n",
    "  \"OUTPUT_FOLDER_NAME\": \"consegna_sindaco_2024\"\n",
    "  // Basta questo! Il dataset Dataiku avr√† un nome generico\n",
    "}\n",
    "```\n",
    "\n",
    "**Caso avanzato** (pipeline Dataiku):\n",
    "```json\n",
    "{\n",
    "  \"OUTPUT_DATASET_NAME\": \"flood_analysis_tevere\",\n",
    "  \"OUTPUT_FOLDER_NAME\": \"export_tevere\" \n",
    "}\n",
    "```\n",
    "\n",
    "**üí° IN PRATICA**: Si consiglia di usare solo `OUTPUT_FOLDER_NAME`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90817b9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
