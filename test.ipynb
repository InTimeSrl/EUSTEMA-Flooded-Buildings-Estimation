{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9bb64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# IMPORT===============================================================================\n",
    "# =====================================================================================\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "# Import third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "\n",
    "# Import Dataiku-specific modules\n",
    "import dataiku\n",
    "from dataiku.scenario import Scenario\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# IMPORTAZIONI LIBRERIE SISTEMA=========================================================\n",
    "# ======================================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "import shutil\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "import importlib\n",
    "# Librerie geospaziali - Stack GDAL/Rasterio\n",
    "try:\n",
    "    import rasterio\n",
    "    from rasterio.transform import from_bounds, Affine\n",
    "    from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "    from rasterio.merge import merge\n",
    "    RASTERIO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RASTERIO_AVAILABLE = False\n",
    "    logging.info(\"ERRORE: Stack rasterio/GDAL non disponibile\")\n",
    "\n",
    "# ======================================================================================\n",
    "# IMPORTAZIONI MODULI SPECIALIZZATI\n",
    "# ======================================================================================\n",
    "\n",
    "try:\n",
    "\n",
    "    from crolli_modules import mosaic_operations\n",
    "    from crolli_modules import alignment_operations\n",
    "    from crolli_modules import resolution_operations\n",
    "    from crolli_modules import io_operations\n",
    "    from crolli_modules import metadata_operations\n",
    "    importlib.reload(io_operations)\n",
    "    EXTERNAL_MODULES_AVAILABLE = True\n",
    "    logging.info(\"Moduli specializzati caricati correttamente\")\n",
    "except ImportError as e:\n",
    "    EXTERNAL_MODULES_AVAILABLE = False\n",
    "    logging.error(f\"ERRORE: Moduli specializzati non disponibili: {e}\")\n",
    "    logging.error(\"Moduli richiesti: mosaic_operations.py, alignment_operations.py,\")\n",
    "    logging.error(\"resolution_operations.py, io_operations.py, metadata_operations.py\")\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# FUNZIONI GENERALI\n",
    "client = dataiku.api_client()\n",
    "project = client.get_default_project()\n",
    "\n",
    "def _set_project_variable(key, value):\n",
    "    variables = project.get_variables()\n",
    "    # Sostituisci NaN (sia float che stringa) con stringa vuota\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)) or str(value).lower() == \"nan\":\n",
    "        value = \"\"\n",
    "    try:\n",
    "        variables['local'][key] = value.strip() if isinstance(value, str) else value\n",
    "    except Exception:\n",
    "        variables['local'][key] = value\n",
    "    project.set_variables(variables)\n",
    "    logging.debug(f\"settata variabile {key} = {value}\")\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "#  CLASSE CONFIGURAZIONE SISTEMA========================================================\n",
    "# ======================================================================================\n",
    "class MergeDatasetConfig:\n",
    "    \"\"\"\n",
    "    Configurazione centralizzata per merge dataset geospaziali.\n",
    "\n",
    "    Gestisce parametri operativi, formati supportati e configurazioni\n",
    "    per garantire compatibilit√† con standard industriali GIS.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, payload: dict = None):\n",
    "        \"\"\"Inizializza configurazione importando parametri globali.\"\"\"\n",
    "        for key, value in globals().items():\n",
    "            if key.isupper() and not key.startswith('_'):\n",
    "                setattr(self, key, value)\n",
    "        self.SUPPORTED_EXTENSIONS = [\n",
    "            \".tif\", \".tiff\",           # GeoTIFF\n",
    "            \".img\",                    # ERDAS Imagine\n",
    "            \".bil\", \".bip\", \".bsq\",    # ENVI band-interleaved\n",
    "            \".adf\",                    # Esri Grid\n",
    "            \".jp2\", \".j2k\", \".jpx\",    # JPEG 2000\n",
    "            \".vrt\",                    # Virtual Raster GDAL\n",
    "            \".hdf\", \".h5\",             # Hierarchical Data Format\n",
    "            \".nc\",                     # NetCDF\n",
    "            \".asc\",                    # ASCII Grid ESRI\n",
    "            \".grd\", \".flt\", \".rst\", \".pix\"  # Formati aggiuntivi\n",
    "        ]\n",
    "\n",
    "        self.MERGE_SUBFOLDER = \"MERGE\"\n",
    "        self.TEMP_SUBFOLDER = \"TEMP_MERGE\"\n",
    "\n",
    "        # Valori sensati di default\n",
    "        self.INPUT_FOLDER_PRE: Optional[str] = None\n",
    "        self.INPUT_FOLDER_POST: Optional[str] = None\n",
    "        self.OUTPUT_FOLDER: Optional[str] = None\n",
    "\n",
    "        # Oggetti Dataiku Folder risolti (se disponibili)\n",
    "        self.INPUT_FOLDER_PRE_OBJ = None\n",
    "        self.INPUT_FOLDER_POST_OBJ = None\n",
    "        self.OUTPUT_FOLDER_OBJ = None\n",
    "\n",
    "        # Percorsi locali temporanei dove abbiamo scaricato le folder (se richiesto)\n",
    "        self.INPUT_FOLDER_PRE_TMP: Optional[str] = None\n",
    "        self.INPUT_FOLDER_POST_TMP: Optional[str] = None\n",
    "        self.OUTPUT_FOLDER_TMP: Optional[str] = None\n",
    "\n",
    "        # Tieni traccia delle tmp dirs create per cleanup\n",
    "        self._tmp_dirs: List[str] = []\n",
    "\n",
    "        self.RESAMPLING_STRATEGY = \"best\"\n",
    "        self.RESAMPLING_METHOD = \"bilinear\"\n",
    "        self.MERGE_METHOD = \"mean\"\n",
    "        self.NODATA_VALUE = -9999\n",
    "        self.ALIGN_PIXELS = True\n",
    "        self.CREATE_HDR_FILES = True\n",
    "        self.COMPRESSION = \"lzw\"\n",
    "        self.TILED = True\n",
    "        self.BLOCK_SIZE = 256\n",
    "        self.OVERWRITE_EXISTING = True\n",
    "        self.ENABLE_NAME_FILTERING = False\n",
    "        self.NAME_FILTER_PATTERN = \"\"\n",
    "\n",
    "        # Rilevazione semplice di moduli esterni utili (rasterio/gdal)\n",
    "        self.EXTERNAL_MODULES_AVAILABLE = False\n",
    "        if 'detect_external_modules' in globals() and detect_external_modules:\n",
    "            try:\n",
    "                import rasterio  # noqa: F401\n",
    "                self.EXTERNAL_MODULES_AVAILABLE = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Dataiku client import (lazy, may not exist in non-DSS env)\n",
    "        self._dataiku_available = False\n",
    "        self._dataiku = None\n",
    "        try:\n",
    "            import dataiku  # type: ignore\n",
    "            self._dataiku_available = True\n",
    "            self._dataiku = dataiku\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(payload)\n",
    "        # Se √® stato passato un payload, parsalo\n",
    "        if payload:\n",
    "            self._load_from_payload(payload)\n",
    "#           \n",
    "    def _load_from_payload(self, payload: dict):\n",
    "\n",
    "        # Campi semplici a livello root: li mappiamo su attributi uppercase con nomi coerenti\n",
    "        mapping = {\n",
    "            \"resampling_strategy\": \"RESAMPLING_STRATEGY\",\n",
    "            \"resampling_method\": \"RESAMPLING_METHOD\",\n",
    "            \"merge_method\": \"MERGE_METHOD\",\n",
    "            \"nodata_value\": \"NODATA_VALUE\",\n",
    "            \"align_pixels\": \"ALIGN_PIXELS\",\n",
    "            \"create_hdr_files\": \"CREATE_HDR_FILES\",\n",
    "            \"compression\": \"COMPRESSION\",\n",
    "            \"tiled\": \"TILED\",\n",
    "            \"block_size\": \"BLOCK_SIZE\",\n",
    "            \"overwrite_existing\": \"OVERWRITE_EXISTING\",\n",
    "            \"enable_name_filtering\": \"ENABLE_NAME_FILTERING\",\n",
    "            \"name_filter_pattern\": \"NAME_FILTER_PATTERN\",\n",
    "\n",
    "            \"damage_threshold\": \"DAMAGE_THRESHOLD\",\n",
    "            \"min_overlap_percent\": \"MIN_OVERLAP_PERCENT\",\n",
    "            \"height_field_name\": \"HEIGHT_FIELD_NAME\",\n",
    "            \"collapse_threshold_percent\": \"COLLAPSE_THRESHOLD_PERCENT\",\n",
    "            \"altezza_build\": \"ALTEZZA_BUILD\",\n",
    "            \"type_costr\": \"TYPE_COSTR\",\n",
    "            \"sup_field\": \"SUP_FIELD\",\n",
    "            \"fid_field\": \"FID_FIELD\",\n",
    "\n",
    "            \"elab_id\": \"ELAB_ID\",\n",
    "            \"input_folder_pre\": \"INPUT_FOLDER_PRE\",\n",
    "            \"input_folder_post\": \"INPUT_FOLDER_POST\",\n",
    "            \"output_folder\": \"OUTPUT_FOLDER\",\n",
    "            \"dsm_pre_path\": \"DSM_PRE_PATH\",\n",
    "            \"dsm_post_path\": \"DSM_POST_PATH\",\n",
    "            \"buildings_path\": \"BUILDINGS_PATH\",\n",
    "            \"output_directory_bdd\": \"OUTPUT_DIRECTORY_BDD\"\n",
    "                }\n",
    "\n",
    "        for src_key, attr in mapping.items():\n",
    "\n",
    "            if src_key in payload:\n",
    "                val = payload[src_key]\n",
    "                print(attr, val)\n",
    "                # Trattamenti specifici\n",
    "                if attr in (\"ALIGN_PIXELS\", \"CREATE_HDR_FILES\", \"TILED\", \"OVERWRITE_EXISTING\", \"ENABLE_NAME_FILTERING\"):\n",
    "                    setattr(self, attr, self._to_bool(val))\n",
    "                elif attr == \"BLOCK_SIZE\":\n",
    "                    setattr(self, attr, self._to_int(val, default=self.BLOCK_SIZE))\n",
    "                else:\n",
    "                    setattr(self, attr, val)\n",
    "\n",
    "\n",
    "    def print_config(self):\n",
    "        \"\"\"Visualizza configurazione corrente del sistema.\"\"\"\n",
    "        print(f\"\\nCORE ENGINE PROCESSING - CONFIGURAZIONE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Dataset PRE:  {(self.INPUT_FOLDER_PRE) if self.INPUT_FOLDER_PRE else 'N/A'}\")\n",
    "        print(f\"Dataset POST: {(self.INPUT_FOLDER_POST) if self.INPUT_FOLDER_POST else 'N/A'}\")\n",
    "        print(f\"Output: {(self.OUTPUT_FOLDER) if self.OUTPUT_FOLDER else 'N/A'}\")\n",
    "        print(f\"\")\n",
    "        print(f\"PARAMETRI PROCESSING:\")\n",
    "        print(f\"  Risoluzione: {self.RESAMPLING_STRATEGY} ({self.RESAMPLING_METHOD})\")\n",
    "        print(f\"  Sovrapposizioni: {self.MERGE_METHOD}\")\n",
    "        print(f\"  NoData: {self.NODATA_VALUE}\")\n",
    "        print(f\"  Allineamento: {'ON' if self.ALIGN_PIXELS else 'OFF'}\")\n",
    "        print(f\"  Metadati HDR: {'ON' if self.CREATE_HDR_FILES else 'OFF'}\")\n",
    "        print(f\"\")\n",
    "        print(f\"SISTEMA:\")\n",
    "        print(f\"  Formati supportati: {len(self.SUPPORTED_EXTENSIONS) if hasattr(self, 'SUPPORTED_EXTENSIONS') else 'N/A'} tipologie\")\n",
    "        print(f\"  Moduli: {'OPERATIVI' if self.EXTERNAL_MODULES_AVAILABLE else 'ERRORE'}\")\n",
    "        print(f\"  Compressione: {self.COMPRESSION.upper()}\")\n",
    "        print(f\"  TIFF: {'TILED' if self.TILED else 'STRIP'} ({self.BLOCK_SIZE}px)\")\n",
    "        if self.ENABLE_NAME_FILTERING:\n",
    "            print(f\"  Filtro naming: {self.NAME_FILTER_PATTERN}\")\n",
    "        else:\n",
    "            print(f\"  Filtro naming: DISABILITATO (tutti i file)\")\n",
    "\n",
    "    def _to_bool(self,val):\n",
    "        \"\"\"Converte diverse rappresentazioni in booleano in modo sicuro.\"\"\"\n",
    "        if isinstance(val, bool):\n",
    "            return val\n",
    "        if val is None:\n",
    "            return False\n",
    "        if isinstance(val, (int, float)):\n",
    "            return bool(val)\n",
    "        s = str(val).strip().lower()\n",
    "        if s in (\"false\", \"0\", \"no\", \"off\", \"\"):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _to_int(self, value, default=0):\n",
    "        \"\"\"Converte un valore in intero, con valore predefinito in caso di errore.\"\"\"\n",
    "        try:\n",
    "            return int(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "\n",
    "    def _is_missing(self, value):\n",
    "        \"\"\"Verifica se un valore √® mancante (None o NaN).\"\"\"\n",
    "        return value is None or (isinstance(value, float) and value != value)\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "#  FUNZIONI COORDINAMENTO MODULI =======================================================\n",
    "# ======================================================================================\n",
    "\n",
    "def determine_utm_zone_for_italy(files_info: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Determina la zona UTM ottimale per il territorio italiano analizzando i bounds dei dataset.\n",
    "\n",
    "    Analizza le coordinate per scegliere tra:\n",
    "    - EPSG:32632 (UTM 32N) per Italia Occidentale\n",
    "    - EPSG:32633 (UTM 33N) per Italia Centro-Orientale\n",
    "\n",
    "    Args:\n",
    "        files_info: Lista metadati files con bounds\n",
    "\n",
    "    Returns:\n",
    "        str: Codice EPSG UTM ottimale per la zona geografica\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Raccogli coordinate geografiche dai file disponibili\n",
    "        geographic_coords = []\n",
    "        files_analyzed = 0\n",
    "        files_with_bounds = 0\n",
    "        files_with_crs = 0\n",
    "\n",
    "        print(f\"   Analisi {len(files_info)} file per determinazione UTM...\")\n",
    "\n",
    "        for file_info in files_info:\n",
    "            files_analyzed += 1\n",
    "            file_name = file_info.get('name', 'Unknown')\n",
    "\n",
    "            # Verifica disponibilit√† bounds\n",
    "            if 'bounds' not in file_info or not file_info['bounds']:\n",
    "                print(f\"   {file_name}: bounds mancanti\")\n",
    "                continue\n",
    "            files_with_bounds += 1\n",
    "\n",
    "            # Verifica disponibilit√† CRS\n",
    "            if 'crs' not in file_info or not file_info['crs']:\n",
    "                print(f\"   {file_name}: CRS mancante\")\n",
    "                continue\n",
    "            files_with_crs += 1\n",
    "\n",
    "            bounds = file_info['bounds']\n",
    "            crs = file_info['crs']\n",
    "\n",
    "            # Calcola centro del dataset\n",
    "            center_x = (bounds[0] + bounds[2]) / 2\n",
    "            center_y = (bounds[1] + bounds[3]) / 2\n",
    "\n",
    "            print(f\"   {file_name}: centro ({center_x:.1f}, {center_y:.1f}), CRS: {str(crs)[:20]}...\")\n",
    "\n",
    "            # Coordinate geografiche WGS84\n",
    "            if (isinstance(crs, object) and hasattr(crs, 'to_epsg') and\n",
    "                crs.to_epsg() == 4326) or str(crs) == 'EPSG:4326':\n",
    "                geographic_coords.append(center_x)  # longitudine\n",
    "                print(f\"      Coordinate geografiche: {center_x:.3f}¬∞E, {center_y:.3f}¬∞N\")\n",
    "            elif (center_x > -180 and center_x < 180 and\n",
    "                  center_y > -90 and center_y < 90):\n",
    "                # Sembrano coordinate geografiche\n",
    "                geographic_coords.append(center_x)\n",
    "                print(f\"   Coordinate geografiche: {center_x:.3f}¬∞, {center_y:.3f}¬∞\")\n",
    "            else:\n",
    "                # Coordinate proiettate - analisi euristica per sistemi italiani\n",
    "                print(f\"   Coordinate proiettate: {center_x:.0f}, {center_y:.0f}\")\n",
    "\n",
    "                # Euristica per sistemi coordinate italiani comuni:\n",
    "                # - UTM 32N Italia Occidentale: X ~ 400,000-700,000\n",
    "                # - UTM 33N Italia Centro-Orientale: X ~ 200,000-600,000\n",
    "                # - Gauss-Boaga Ovest: X ~ 1,400,000-1,700,000\n",
    "                # - Gauss-Boaga Est: X ~ 2,300,000-2,700,000\n",
    "\n",
    "                if 1400000 <= center_x <= 1700000:\n",
    "                    # Gauss-Boaga Ovest ‚Üí UTM 32N\n",
    "                    geographic_coords.append(9.0)  # Approssimazione Italia Occidentale\n",
    "                    print(f\"   Gauss-Boaga Ovest rilevato ‚Üí UTM 32N suggerito\")\n",
    "                elif 2300000 <= center_x <= 2700000:\n",
    "                    # Gauss-Boaga Est ‚Üí UTM 33N\n",
    "                    geographic_coords.append(15.0)  # Approssimazione Italia Centro-Orientale\n",
    "                    print(f\"   Gauss-Boaga Est rilevato ‚Üí UTM 33N suggerito\")\n",
    "                elif 400000 <= center_x <= 700000:\n",
    "                    # Potenziale UTM 32N\n",
    "                    geographic_coords.append(9.0)\n",
    "                    print(f\"   Possibile UTM 32N ‚Üí confermato\")\n",
    "                elif 200000 <= center_x <= 600000:\n",
    "                    # Potenziale UTM 33N\n",
    "                    geographic_coords.append(15.0)\n",
    "                    print(f\"   Possibile UTM 33N ‚Üí confermato\")\n",
    "        # Report diagnostico\n",
    "        print(f\"   Diagnostica: {files_analyzed} file analizzati, {files_with_bounds} con bounds, {files_with_crs} con CRS\")\n",
    "        print(f\"   Coordinate geografiche raccolte: {len(geographic_coords)}\")\n",
    "\n",
    "        if not geographic_coords:\n",
    "            print(\"   Nessuna coordinata analizzabile, uso default UTM 32N\")\n",
    "            return \"EPSG:32632\"\n",
    "\n",
    "        # Calcola longitudine media\n",
    "        avg_longitude = sum(geographic_coords) / len(geographic_coords)\n",
    "        print(f\"   Longitudine media stimata: {avg_longitude:.1f}¬∞\")\n",
    "\n",
    "        # Decisione finale zona UTM per Italia\n",
    "        if avg_longitude < 12.0:\n",
    "            selected_utm = \"EPSG:32632\"  # UTM 32N\n",
    "            zone_name = \"UTM 32N (Italia Occidentale)\"\n",
    "        else:\n",
    "            selected_utm = \"EPSG:32633\"  # UTM 33N\n",
    "            zone_name = \"UTM 33N (Italia Centro-Orientale)\"\n",
    "\n",
    "        print(f\"   UTM selezionato: {selected_utm} - {zone_name}\")\n",
    "        return selected_utm\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Errore determinazione UTM: {e}\")\n",
    "        print(\"   Fallback default: EPSG:32632 (UTM 32N)\")\n",
    "        return \"EPSG:32632\"\n",
    "\n",
    "def get_crs_from_first_file(files_info: List[Dict]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Estrae CRS dal primo file di un dataset in modo semplice e diretto.\n",
    "\n",
    "    Args:\n",
    "        files_info: Lista metadati files da io_operations.analyze_datasets()\n",
    "\n",
    "    Returns:\n",
    "        str: CRS in formato string (es. \"EPSG:25832\") o None se non trovato\n",
    "    \"\"\"\n",
    "    if not files_info:\n",
    "        return None\n",
    "\n",
    "    first_file = files_info[0]\n",
    "    crs = first_file.get('crs')\n",
    "\n",
    "    if crs is not None:\n",
    "        crs_string = str(crs)\n",
    "        print(f\"   CRS estratto: {crs_string}\")\n",
    "        return crs_string\n",
    "    else:\n",
    "        print(f\"   CRS non trovato nel file: {first_file.get('path', 'Unknown')}\")\n",
    "        return None\n",
    "\n",
    "def manage_crs_simplified(pre_info: List[Dict], post_info: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Gestione CRS automatica con priorit√† PRE ‚Üí POST ‚Üí fallback geografico.\n",
    "\n",
    "    Logica di priorit√†:\n",
    "    1. Se PRE ha CRS valido ‚Üí usa quello (priorit√† assoluta)\n",
    "    2. Se PRE non ha CRS ma POST s√¨ ‚Üí usa CRS di POST\n",
    "    3. Altrimenti ‚Üí determina UTM ottimale per Italia basandosi sulla geografia\n",
    "\n",
    "    Args:\n",
    "        pre_info: Metadati dataset PRE da io_operations\n",
    "        post_info: Metadati dataset POST da io_operations\n",
    "\n",
    "    Returns:\n",
    "        str: CRS target per processing (sempre valido)\n",
    "    \"\"\"\n",
    "    print(\"Gestione CRS con priorit√† PRE ‚Üí POST ‚Üí geografico...\")\n",
    "\n",
    "    # PRIORIT√Ä 1: Estrai CRS da dataset PRE\n",
    "    print(\"Estrazione CRS dataset PRE...\")\n",
    "    pre_crs = get_crs_from_first_file(pre_info)\n",
    "\n",
    "    if pre_crs:\n",
    "        print(f\"PRE ha CRS valido: {pre_crs} ‚Üí UTILIZZATO (priorit√† assoluta)\")\n",
    "        return pre_crs\n",
    "\n",
    "    print(\"PRE senza CRS, controllo POST...\")\n",
    "\n",
    "    # PRIORIT√Ä 2: Se PRE non ha CRS, prova POST\n",
    "    print(\"Estrazione CRS dataset POST...\")\n",
    "    post_crs = get_crs_from_first_file(post_info)\n",
    "\n",
    "    if post_crs:\n",
    "        print(f\"POST ha CRS valido: {post_crs} ‚Üí UTILIZZATO (fallback POST)\")\n",
    "        return post_crs\n",
    "\n",
    "    print(\"N√© PRE n√© POST hanno CRS validi\")\n",
    "\n",
    "    # PRIORIT√Ä 3: Fallback intelligente basato su geografia\n",
    "    print(\"Determinazione UTM ottimale per Italia...\")\n",
    "\n",
    "    # Usa tutti i file disponibili per determinare la zona geografica\n",
    "    all_files = pre_info + post_info\n",
    "    fallback_crs = determine_utm_zone_for_italy(all_files)\n",
    "\n",
    "    print(f\"Fallback geografico: {fallback_crs}\")\n",
    "    return fallback_crs\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# WORKFLOW PRINCIPALE SISTEMA===========================================================\n",
    "# ======================================================================================\n",
    "def run_workflow(payload):\n",
    "    \"\"\"\n",
    "    Workflow principale per elaborazione dataset DSM/DTM.\n",
    "\n",
    "    Esegue il pipeline di processing attraverso l'architettura modulare:\n",
    "    1. Verifica dipendenze sistema\n",
    "    2. Scansione dataset input\n",
    "    3. Analisi metadati e validazione\n",
    "    4. Gestione sistemi coordinate (CRS)\n",
    "    5. Mosaicatura dataset\n",
    "    6. Allineamento pixel\n",
    "    7. Generazione metadati professionali\n",
    "\n",
    "    Returns:\n",
    "        tuple: (success: bool, output_files_count: int) - True/numero file se completato con successo, False/0 in caso di errori\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CORE ENGINE PROCESSING - AVVIO SISTEMA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        # Verifica dipendenze\n",
    "        print(\"\\nVerifica dipendenze sistema...\")\n",
    "\n",
    "        if not RASTERIO_AVAILABLE:\n",
    "            print(\"ERRORE: Stack rasterio/GDAL non disponibile\")\n",
    "            return False, 0\n",
    "\n",
    "        if not EXTERNAL_MODULES_AVAILABLE:\n",
    "            print(\"ERRORE: Moduli specializzati non disponibili\")\n",
    "            return False, 0\n",
    "\n",
    "        print(\"Dipendenze verificate correttamente\")\n",
    "\n",
    "        # Inizializzazione configurazione\n",
    "        print(\"\\nInizializzazione configurazione...\")\n",
    "        config = MergeDatasetConfig(payload)\n",
    "\n",
    "        config.print_config()\n",
    "\n",
    "#         # Estrai i valori dinamicamente da config.OUTPUT_FOLDER\n",
    "#         output_base, output_subfolder = os.path.split(config.OUTPUT_FOLDER)\n",
    "\n",
    "#         # Usa una directory temporanea per il percorso base\n",
    "#         if not os.path.isabs(output_base):\n",
    "#             output_base = os.path.join(tempfile.gettempdir(), output_base.lstrip('/'))\n",
    "\n",
    "#         # Aggiorna il percorso di OUTPUT_FOLDER\n",
    "#         config.OUTPUT_FOLDER = os.path.join(output_base, output_subfolder)\n",
    "#         Path(config.OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         print(f\"‚úÖ Directory di output configurata dinamicamente: {config.OUTPUT_FOLDER}\")\n",
    "\n",
    "\n",
    "        # FASE 1: Scansione dataset\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FASE 1/6: SCANSIONE DATASET\")\n",
    "        print(\"=\"*60)\n",
    "        folder = dataiku.Folder(\"minio_input\")\n",
    "        file_list = folder.list_paths_in_partition()\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            pre_folder = os.path.join(tmpdir, \"pre_folder\")\n",
    "            post_folder = os.path.join(tmpdir, \"post_folder\")\n",
    "            temp_output_folder =os.path.join(tmpdir, \"output\")\n",
    "\n",
    "\n",
    "            os.makedirs(pre_folder, exist_ok=True)\n",
    "            os.makedirs(post_folder, exist_ok=True)\n",
    "\n",
    "            pre_file_found = False\n",
    "            post_file_found = False\n",
    "\n",
    "            print(f\"Cartella temporanea creata: {tmpdir}\")\n",
    "\n",
    "            for f in file_list:\n",
    "                print(f.lower())\n",
    "                print(config.INPUT_FOLDER_PRE)\n",
    "                if f:\n",
    "                    if f.lower() == config.INPUT_FOLDER_PRE.lower():\n",
    "                        pre_file = os.path.join(pre_folder, os.path.basename(f))\n",
    "                        with folder.get_download_stream(f) as stream, open(pre_file, 'wb') as out:\n",
    "                            out.write(stream.read())\n",
    "                        pre_file_found = True\n",
    "                        print(f\"‚úÖ File scaricato in pre_folder: {pre_file}\")\n",
    "\n",
    "                    if f.lower() == config.INPUT_FOLDER_POST.lower():\n",
    "                        post_file = os.path.join(post_folder, os.path.basename(f))\n",
    "                        with folder.get_download_stream(f) as stream, open(post_file, 'wb') as out:\n",
    "                            out.write(stream.read())\n",
    "                        post_file_found = True\n",
    "                        print(f\"‚úÖ File scaricato in post_folder: {post_file}\")\n",
    "\n",
    "            # Verifica il contenuto delle directory\n",
    "            print(\"Contenuto di pre_folder:\")\n",
    "            print(os.listdir(pre_folder))\n",
    "            print(\"Contenuto di post_folder:\")\n",
    "            print(os.listdir(post_folder))\n",
    "\n",
    "            # Verifica se i file sono stati scaricati\n",
    "            if pre_file_found and os.path.isdir(pre_folder):\n",
    "                config.INPUT_FOLDER_PRE = pre_folder\n",
    "                print(f\"‚úÖ Percorso valido per INPUT_FOLDER_PRE: {pre_folder}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Percorso non trovato 111: {pre_folder}\")\n",
    "                raise ValueError(\"File per INPUT_FOLDER_PRE non trovato o directory non valida.\")\n",
    "\n",
    "            if post_file_found and os.path.isdir(post_folder):\n",
    "                config.INPUT_FOLDER_POST = post_folder\n",
    "                print(f\"‚úÖ Percorso valido per INPUT_FOLDER_POST: {post_folder}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Percorso non trovato 222: {post_folder}\")\n",
    "                raise ValueError(\"File per INPUT_FOLDER_POST non trovato o directory non valida.\")\n",
    "\n",
    "            # Sposta il codice che utilizza pre_folder_or_path e post_folder_or_path qui\n",
    "            pre_folder_or_path = config.INPUT_FOLDER_PRE\n",
    "            post_folder_or_path = config.INPUT_FOLDER_POST\n",
    "\n",
    "            if os.path.isdir(pre_folder_or_path):\n",
    "                print(\"Contenuto di pre_folder_or_path:\")\n",
    "                for item in os.listdir(pre_folder_or_path):\n",
    "                    print(f\"  - {item}\")\n",
    "            else:\n",
    "                print(f\"pre_folder_or_path non √® una directory: {pre_folder_or_path}\")\n",
    "\n",
    "            if os.path.isdir(post_folder_or_path):\n",
    "                print(\"Contenuto di post_folder_or_path:\")\n",
    "                for item in os.listdir(post_folder_or_path):\n",
    "                    print(f\"  - {item}\")\n",
    "            else:\n",
    "                print(f\"post_folder_or_path non √® una directory: {post_folder_or_path}\")\n",
    "\n",
    "            # ottieni i percorsi locali da passare a io_operations\n",
    "            pre_folder_or_path = config.INPUT_FOLDER_PRE\n",
    "            post_folder_or_path = config.INPUT_FOLDER_POST\n",
    "            print(f\"pre_folder_or_path:  {pre_folder_or_path}\")\n",
    "            print(f\"post_folder_or_path:  {post_folder_or_path}\")\n",
    "\n",
    "\n",
    "            pre_files = io_operations.scan_dataset_files(pre_folder_or_path, config)\n",
    "            post_files = io_operations.scan_dataset_files(post_folder_or_path, config)\n",
    "\n",
    "            if not pre_files or not post_files:\n",
    "                print(\"ERRORE: Dataset PRE o POST non contengono file validi\")\n",
    "                print(f\"Files PRE: {len(pre_files) if pre_files else 0}\")\n",
    "                print(f\"Files POST: {len(post_files) if post_files else 0}\")\n",
    "                return False, 0\n",
    "\n",
    "            print(f\"Scansione completata\")\n",
    "            print(f\"Dataset PRE: {len(pre_files)} file\")\n",
    "            print(f\"Dataset POST: {len(post_files)} file\")\n",
    "\n",
    "            # FASE 2: Analisi metadati\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FASE 2/6: ANALISI METADATI\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            print(\"Analisi metadati dataset...\")\n",
    "\n",
    "            pre_info, post_info = io_operations.analyze_datasets(pre_files, post_files, config)\n",
    "\n",
    "            if not pre_info or not post_info:\n",
    "                print(\"ERRORE: Impossibile analizzare metadati dataset\")\n",
    "                return False, 0\n",
    "\n",
    "            print(\"Analisi metadati completata\")\n",
    "            print(f\"Dataset PRE: {len(pre_info)} file analizzati\")\n",
    "            print(f\"Dataset POST: {len(post_info)} file analizzati\")\n",
    "\n",
    "            # FASE 3: Gestione CRS\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FASE 3/6: GESTIONE SISTEMI COORDINATE\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            print(\"Gestione sistemi coordinate...\")\n",
    "\n",
    "            target_crs = manage_crs_simplified(pre_info, post_info)\n",
    "            # NOTA: La nuova gestione restituisce SEMPRE un CRS valido (con fallback automatico)\n",
    "\n",
    "            print(\"Gestione CRS completata\")\n",
    "            print(f\"Sistema coordinate target: {target_crs}\")\n",
    "\n",
    "            # FASE 3.5: Determinazione risoluzione spaziale\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FASE 3.5/6: DETERMINAZIONE RISOLUZIONE SPAZIALE\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            print(\"Analisi risoluzioni dataset...\")\n",
    "\n",
    "            # Estrai risoluzione rappresentativa per PRE e POST\n",
    "            pre_resolution = resolution_operations.get_most_frequent_resolution(pre_info)\n",
    "            post_resolution = resolution_operations.get_most_frequent_resolution(post_info)\n",
    "\n",
    "            if pre_resolution is None or post_resolution is None:\n",
    "                print(\"ERRORE: Impossibile estrarre risoluzioni dai dataset\")\n",
    "                return False, 0\n",
    "\n",
    "            print(f\"Risoluzione PRE dominante: {pre_resolution:.3f}m\")\n",
    "            print(f\"Risoluzione POST dominante: {post_resolution:.3f}m\")\n",
    "\n",
    "            # Determina risoluzione finale usando resolution_operations\n",
    "            target_resolution = resolution_operations.determine_final_resolution_between_datasets(\n",
    "                pre_resolution, post_resolution, config\n",
    "            )\n",
    "\n",
    "            if target_resolution is None:\n",
    "                print(\"ERRORE: Impossibile determinare risoluzione target\")\n",
    "                return False, 0\n",
    "\n",
    "            print(f\"Risoluzione target determinata: {target_resolution}m\")\n",
    "            print(f\"Strategia utilizzata: {config.RESAMPLING_STRATEGY}\")\n",
    "\n",
    "            # FASE 4: Mosaicatura\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FASE 4/6: MOSAICATURA DATASET\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            print(\"Mosaicatura dataset...\")\n",
    "\n",
    "#             Path(config.OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "#             merge_folder = Path(config.OUTPUT_FOLDER) / config.MERGE_SUBFOLDER\n",
    "            temp_output_folder = Path(temp_output_folder)\n",
    "            merge_folder = temp_output_folder / config.MERGE_SUBFOLDER\n",
    "#             merge_folder.mkdir(exist_ok=True)\n",
    "            os.makedirs(merge_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "            pre_mosaic_path = str(merge_folder / \"merged_PRE.tif\")\n",
    "            post_mosaic_path = str(merge_folder / \"merged_POST.tif\")\n",
    "\n",
    "            print(f\"Output: {merge_folder}\")\n",
    "\n",
    "            pre_result = mosaic_operations.create_mosaic(pre_info, pre_mosaic_path, target_crs, config, \"PRE\", target_resolution)\n",
    "            post_result = mosaic_operations.create_mosaic(post_info, post_mosaic_path, target_crs, config, \"POST\", target_resolution)\n",
    "\n",
    "            if not pre_result or not post_result:\n",
    "                print(\"ERRORE: Fallimento mosaicatura\")\n",
    "                return False, 0\n",
    "\n",
    "            print(\"Mosaicatura completata\")\n",
    "            print(f\"Dataset PRE: {Path(pre_result).name}\")\n",
    "            print(f\"Dataset POST: {Path(post_result).name}\")\n",
    "\n",
    "            # FASE 5: Allineamento\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FASE 5/6: ALLINEAMENTO GRIGLIA PIXEL\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            if config.ALIGN_PIXELS:\n",
    "                print(\"Allineamento griglia pixel...\")\n",
    "\n",
    "                try:\n",
    "                    aligned_pre, aligned_post = alignment_operations.align_grids(pre_result, post_result, config)\n",
    "                    if aligned_pre and aligned_post:\n",
    "                        pre_result, post_result = aligned_pre, aligned_post\n",
    "                        print(\"Allineamento completato\")\n",
    "                    else:\n",
    "                        print(\"Allineamento non riuscito, utilizzo mosaici originali\")\n",
    "                except ValueError as e:\n",
    "                    if \"Nessuna sovrapposizione geometrica\" in str(e):\n",
    "                        print(\"\\nüö® PROCESSO INTERROTTO: NESSUNA SOVRAPPOSIZIONE GEOMETRICA\")\n",
    "                        print(\"Il workflow √® stato terminato in modo controllato.\")\n",
    "                        print(\"Consultare i dettagli sopra per la risoluzione del problema.\")\n",
    "                        return False, 0\n",
    "                    else:\n",
    "                        # Altro tipo di ValueError, ri-solleva\n",
    "                        raise\n",
    "            else:\n",
    "                print(\"Allineamento disattivato\")\n",
    "\n",
    "            # FASE 6: Metadati\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FASE 6/6: GENERAZIONE METADATI\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            if config.CREATE_HDR_FILES:\n",
    "                print(\"Generazione metadati HDR...\")\n",
    "\n",
    "                metadata_operations.create_hdr_files(pre_result, post_result, config)\n",
    "\n",
    "                print(\"Metadati HDR generati\")\n",
    "            else:\n",
    "                print(\"Generazione metadati disattivata\")\n",
    "\n",
    "            # Report finale\n",
    "            processing_time = time.time() - start_time\n",
    "\n",
    "            # Conta i file di output generati\n",
    "            output_files = []\n",
    "#             output_folder = Path(config.OUTPUT_FOLDER)\n",
    "\n",
    "            # File principali (.tif)\n",
    "            if os.path.exists(pre_result):\n",
    "                output_files.append(pre_result)\n",
    "            if os.path.exists(post_result):\n",
    "                output_files.append(post_result)\n",
    "\n",
    "            # File metadati (.hdr)\n",
    "            if config.CREATE_HDR_FILES:\n",
    "                pre_hdr = pre_result.replace('.tif', '.hdr')\n",
    "                post_hdr = post_result.replace('.tif', '.hdr')\n",
    "                if os.path.exists(pre_hdr):\n",
    "                    output_files.append(pre_hdr)\n",
    "                if os.path.exists(post_hdr):\n",
    "                    output_files.append(post_hdr)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ELABORAZIONE COMPLETATA\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Dataset PRE:  {Path(pre_result).name}\")\n",
    "            print(f\"Dataset POST: {Path(post_result).name}\")\n",
    "            print(f\"Tempo elaborazione: {processing_time:.1f} secondi\")\n",
    "            print(f\"Sistema: Core Engine Processing\")\n",
    "\n",
    "            # Esporta i file dalla cartella temporanea alla cartella di output con Dataiku\n",
    "            output_folder = dataiku.Folder(\"output_preprocessing_raster\")\n",
    "            output_folder_path = output_folder.get_path()\n",
    "\n",
    "            # Determina codice di partition (es. \"202509231300\") a partire dal percorso originale\n",
    "            base_code = None\n",
    "            base_code = payload.get('elab_id')\n",
    "\n",
    "            # Create target partitioned MERGE folder inside managed folder\n",
    "            partition_merge_dir = os.path.join(output_folder_path, base_code, config.MERGE_SUBFOLDER)\n",
    "            os.makedirs(partition_merge_dir, exist_ok=True)\n",
    "\n",
    "            # Copia file e directory dal temp_output_folder nella cartella partitioned/MERGE\n",
    "            for item_name in os.listdir(temp_output_folder):\n",
    "                src_path = os.path.join(temp_output_folder, item_name)\n",
    "                dest_path = os.path.join(partition_merge_dir, item_name)\n",
    "\n",
    "                if os.path.isfile(src_path):\n",
    "                    shutil.copy2(src_path, dest_path)\n",
    "                elif os.path.isdir(src_path):\n",
    "                    shutil.copytree(src_path, dest_path, dirs_exist_ok=True)\n",
    "\n",
    "            print(f\"‚úÖ Dati esportati (file e directory) nella cartella di output partitioned: {partition_merge_dir}\")\n",
    "\n",
    "            # --- NUOVA LOGICA: impostazione variabili locali per la recipe successiva ---\n",
    "            try:\n",
    "                # Costruisci link relativi usati dalla recipe successiva (partition/MERGE/filename)\n",
    "                pre_filename = os.path.basename(pre_result) if pre_result else None\n",
    "                post_filename = os.path.basename(post_result) if post_result else None\n",
    "\n",
    "                if pre_filename:\n",
    "                    dsm_pre_link = os.path.join(base_code, config.MERGE_SUBFOLDER, pre_filename)\n",
    "                    # _set_project_variable(\"DSM_PRE_PATH\", dsm_pre_link)\n",
    "                    config.DSM_PRE_PATH = dsm_pre_link\n",
    "                else:\n",
    "                    dsm_pre_link = None\n",
    "\n",
    "                if post_filename:\n",
    "                    dsm_post_link = os.path.join(base_code, config.MERGE_SUBFOLDER, post_filename)\n",
    "                    # _set_project_variable(\"DSM_POST_PATH\", dsm_post_link)\n",
    "                    config.DSM_POST_PATH = dsm_post_link\n",
    "                else:\n",
    "                    dsm_post_link = None\n",
    "\n",
    "                print(f\"‚úÖ Variabili locali impostate:\")\n",
    "                if dsm_pre_link:\n",
    "                    print(f\"   DSM_PRE_PATH = {dsm_pre_link}\")\n",
    "                if dsm_post_link:\n",
    "                    print(f\"   DSM_POST_PATH = {dsm_post_link}\")\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(f\"‚ö†Ô∏è Errore impostazione variabili locali: {ex}\")\n",
    "            # --- fine nuova logica ---\n",
    "\n",
    "\n",
    "            return True, len(output_files), config\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERRORE SISTEMA: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return False, 0\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "def _create_payload_from_config_tables():\n",
    "    payload = {}\n",
    "\n",
    "    # --- leggi tabella 'configurazione'\n",
    "    try:\n",
    "        conf_df = dataiku.Dataset('configurazione').get_dataframe()\n",
    "        for _, row in conf_df.iterrows():\n",
    "            var_name = row.get(\"variabile\")\n",
    "            var_value = row.get(\"valore\")\n",
    "\n",
    "            # prova a interpretare JSON nel valore\n",
    "            if isinstance(var_value, str):\n",
    "                try:\n",
    "                    var_value = json.loads(var_value)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if var_name:  # aggiungi solo se esiste il nome\n",
    "                payload[var_name] = var_value\n",
    "    except Exception as ex:\n",
    "        logging.warning(f\"Errore lettura tabella 'configurazione': {ex}\")\n",
    "\n",
    "    logging.debug(\"Creazione del payload completata\")\n",
    "    return payload\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "def _prepare_scenario_payload(payload: dict):\n",
    "    \"\"\"\n",
    "    Prepara il payload da scenario.\n",
    "    Normalizza chiavi maiuscole/minuscole,\n",
    "    converte booleani e gestisce nan come None.\n",
    "    \"\"\"\n",
    "\n",
    "    def norm_val(v):\n",
    "        \"\"\"Converte stringhe True/False, nan ‚Üí None\"\"\"\n",
    "        if isinstance(v, str):\n",
    "            lv = v.strip().lower()\n",
    "            if lv in (\"true\", \"1\", \"yes\", \"y\"):\n",
    "                return True\n",
    "            if lv in (\"false\", \"0\", \"no\", \"n\"):\n",
    "                return False\n",
    "        if isinstance(v, float) and math.isnan(v):\n",
    "            return None\n",
    "        return v\n",
    "\n",
    "    # normalizza tutte le chiavi a lowercase per uniformit√†\n",
    "    norm_payload = {k.lower(): norm_val(v) for k, v in payload.items()}\n",
    "\n",
    "    fixed_payload = {\n",
    "        \"elab_id\": norm_payload.get(\"elab_id\", \"\"),\n",
    "        \"input_folder_pre\": norm_payload.get(\"input_folder_pre\"),\n",
    "        \"input_folder_post\": norm_payload.get(\"input_folder_post\"),\n",
    "        \"output_folder\": norm_payload.get(\"output_folder\"),\n",
    "        \"dsm_pre_path\": norm_payload.get(\"dsm_pre_path\"),\n",
    "        \"dsm_post_path\": norm_payload.get(\"dsm_post_path\"),\n",
    "        \"buildings_path\": norm_payload.get(\"buildings_path\"),\n",
    "        \"output_directory\": norm_payload.get(\"output_directory\"),\n",
    "\n",
    "        \"variabili\": norm_payload.get(\"variabili\", []),\n",
    "\n",
    "        \"resampling_strategy\": norm_payload.get(\"resampling_strategy\", \"best\"),\n",
    "        \"resampling_method\": norm_payload.get(\"resampling_method\", \"bilinear\"),\n",
    "        \"merge_method\": norm_payload.get(\"merge_method\", \"mean\"),\n",
    "        \"nodata_value\": norm_payload.get(\"nodata_value\", -9999),\n",
    "        \"align_pixels\": norm_payload.get(\"align_pixels\", False),\n",
    "        \"create_hdr_files\": norm_payload.get(\"create_hdr_files\", False),\n",
    "        \"compression\": norm_payload.get(\"compression\", \"lzw\"),\n",
    "        \"tiled\": norm_payload.get(\"tiled\", False),\n",
    "        \"block_size\": norm_payload.get(\"block_size\", 256),\n",
    "        \"overwrite_existing\": norm_payload.get(\"overwrite_existing\", False),\n",
    "        \"enable_name_filtering\": norm_payload.get(\"enable_name_filtering\", False),\n",
    "        \"name_filter_pattern\": norm_payload.get(\"name_filter_pattern\", \"\"),\n",
    "\n",
    "        # campi aggiuntivi specifici\n",
    "        \"damage_threshold\": norm_payload.get(\"damage_threshold\", 0.5),\n",
    "        \"min_overlap_percent\": norm_payload.get(\"min_overlap_percent\", 50.0),\n",
    "        \"height_field_name\": norm_payload.get(\"height_field_name\", \"altezza\"),\n",
    "        \"collapse_threshold_percent\": norm_payload.get(\"collapse_threshold_percent\", 50.0),\n",
    "        \"altezza_build\": norm_payload.get(\"altezza_build\"),\n",
    "        \"type_costr\": norm_payload.get(\"type_costr\"),\n",
    "        \"sup_field\": norm_payload.get(\"sup_field\"),\n",
    "        \"fid_field\": norm_payload.get(\"fid_field\"),\n",
    "\n",
    "        # placeholders per future espansioni\n",
    "        \"aree_evento\": [],\n",
    "        \"inquadramento\": []\n",
    "    }\n",
    "\n",
    "    return fixed_payload\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "def _create_payload():\n",
    "    payload = None\n",
    "    try:\n",
    "        run_vars = dataiku.get_custom_variables()\n",
    "        logging.debug(f\"recuperato variabili custom\")\n",
    "        scenario_run_id = run_vars.get('scenarioTriggerRunId')\n",
    "        if scenario_run_id is not None:\n",
    "            logging.info(f\"avvio recipe da scenario{scenario_run_id}\")\n",
    "            payload = json.loads(run_vars.get('scenarioTriggerParams'))\n",
    "            payload = _prepare_scenario_payload(payload)\n",
    "            if payload.get('elab_id') is None:\n",
    "                payload[\"elab_id\"] = f\"e{scenario_run_id.replace('-', '')[:-3]}\"\n",
    "            else:\n",
    "                payload[\"elab_id\"] = f\"e{payload['elab_id']}\"\n",
    "        else:\n",
    "            logging.info(f\"avvio recipe da flow\")\n",
    "            payload = _create_payload_from_config_tables()\n",
    "        logging.debug(f\"{json.dumps(payload)}\")\n",
    "\n",
    "        try:\n",
    "            skip_keys = {'variabili', 'aree_evento', 'inquadramento'}\n",
    "            for k, v in payload.items():\n",
    "                if k in skip_keys:\n",
    "                    continue\n",
    "                # scrivi solo valori semplici\n",
    "                if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "                    _set_project_variable(k.upper(), v)\n",
    "                    # Imposta le variabili esplicite fornite in 'variabili'\n",
    "            vars_list = payload.get('variabili') or []\n",
    "            if isinstance(vars_list, dict):\n",
    "                vars_list = [vars_list]\n",
    "            for item in vars_list:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                name = item.get('nome_variabile')\n",
    "                val = item.get('percorso_variabile')\n",
    "                if name:\n",
    "                    _set_project_variable(name, val)\n",
    "\n",
    "            logging.info(f\"Variabili locali impostate da payload:\")\n",
    "        except Exception as ex:\n",
    "            logging.warning(f\"Errore impostando variabili progetto da payload: {ex}\")\n",
    "        return payload\n",
    "    except Exception as ex:\n",
    "        logging.error(f\"{ex}\")\n",
    "        raise\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "def main():\n",
    "    payload = _create_payload()\n",
    "    print(payload)\n",
    "    if payload is None:\n",
    "        logging.error(\"Errore nella creazione del payload\")\n",
    "        raise Exception(\"Errore nella creazione del payload\")\n",
    "    try:\n",
    "        print(\"CORE ENGINE PROCESSING\")\n",
    "        print(\"Architettura Modulare\")\n",
    "        print(\"Sistema di Elaborazione Geospaziale\")\n",
    "\n",
    "        success, output_files_count, config = run_workflow(payload)\n",
    "\n",
    "        # Ripristina stdout originale per messaggio finale\n",
    "#         sys.stdout = original_stdout\n",
    "\n",
    "        if not success:\n",
    "            print(f\"‚ùå ELABORAZIONE FALLITA - Exit Code: 1\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        print(f\"‚úÖ ELABORAZIONE DATASET COMPLETATA - Exit Code: 0\")\n",
    "        print(f\"üìÅ Output files: {output_files_count} generati\")\n",
    "        print(config)\n",
    "        if config:\n",
    "            print(config)\n",
    "\n",
    "            # Prepara una riga risultato serializzabile dal config\n",
    "            results_row = {\n",
    "                \"elab_id\": getattr(config, \"ELAB_ID\", None) or getattr(config, \"elab_id\", None),\n",
    "                \"dsm_pre_path\": getattr(config, \"DSM_PRE_PATH\", None),\n",
    "                \"dsm_post_path\": getattr(config, \"DSM_POST_PATH\", None),\n",
    "                \"output_folder\": getattr(config, \"OUTPUT_FOLDER\", None),\n",
    "                \"merge_subfolder\": getattr(config, \"MERGE_SUBFOLDER\", None),\n",
    "                \"resampling_strategy\": getattr(config, \"RESAMPLING_STRATEGY\", None),\n",
    "                \"resampling_method\": getattr(config, \"RESAMPLING_METHOD\", None),\n",
    "                \"merge_method\": getattr(config, \"MERGE_METHOD\", None),\n",
    "                \"nodata_value\": getattr(config, \"NODATA_VALUE\", None),\n",
    "                \"align_pixels\": getattr(config, \"ALIGN_PIXELS\", None),\n",
    "                \"create_hdr_files\": getattr(config, \"CREATE_HDR_FILES\", None),\n",
    "                \"compression\": getattr(config, \"COMPRESSION\", None),\n",
    "                \"tiled\": getattr(config, \"TILED\", None),\n",
    "                \"block_size\": getattr(config, \"BLOCK_SIZE\", None),\n",
    "                \"overwrite_existing\": getattr(config, \"OVERWRITE_EXISTING\", None),\n",
    "            }\n",
    "            try:\n",
    "                df_results = pd.DataFrame([results_row])\n",
    "                db_results = dataiku.Dataset(\"db_results\")\n",
    "                db_results.write_with_schema(df_results)\n",
    "                print(\"‚úÖ db_results aggiornato\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Impossibile scrivere db_results: {e}\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Ripristina stdout in caso di errore critico\n",
    "#         sys.stdout = original_stdout\n",
    "        print(f\"‚ùå ERRORE CRITICO - Exit Code: 1\")\n",
    "        print(f\"Errore: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# AVVIA MAIN\n",
    "main()\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# Recipe outputs\n",
    "# vxkciquq = dataiku.Folder(\"vxKCiqUq\")\n",
    "# vxkciquq_info = vxkciquq.get_info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
